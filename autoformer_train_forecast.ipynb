{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'Autoformer' in sys.path[0]:\n",
    "    sys.path[0] += '/Autoformer'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30:17:14:20,249 INFO     [utils.py:147] Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-11-30:17:14:20,250 INFO     [utils.py:159] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021 \n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First experiment - prediction length 24\n",
    "Set arguements for our model :<br>\n",
    "Data - Synthetic dataset 1<br>\n",
    "Model - Autoformer<br>\n",
    "Frequency - hourly<br>\n",
    "Features - univariate\n",
    "Encoder length - 96<br>\n",
    "Label length - 48<br>\n",
    "Prediction horizon - 24<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'target': 'TARGET', 'des': 'Exp', 'dropout': 0.05, 'num_workers': 10, 'freq': 'h', 'checkpoints': './checkpoints/', 'bucket_size': 4, 'n_hashes': 4, 'is_trainging': True, 'root_path': './SYNTHDataset', 'data_path': 'SYNTHh1.csv', 'model_id': 'Synth1_96_24', 'model': 'Autoformer', 'data': 'Synth1', 'features': 'S', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'e_layers': 2, 'd_layers': 1, 'n_heads': 8, 'factor': 1, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'd_model': 512, 'itr': 1, 'd_ff': 2048, 'moving_avg': 25, 'distil': True, 'output_attention': False, 'patience': 3, 'learning_rate': 0.0001, 'batch_size': 32, 'embed': 'timeF', 'activation': 'gelu', 'use_amp': False, 'loss': 'mse', 'train_epochs': 10, 'gpu': 0, 'lradj': 'type1', 'devices': '0,1,2,3', 'use_multi_gpu': False, 'use_gpu': True}\n"
     ]
    }
   ],
   "source": [
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "args.target = 'TARGET'\n",
    "args.des = 'test'\n",
    "args.dropout = 0.05\n",
    "args.num_workers = 10\n",
    "args.freq = 'h'\n",
    "args.checkpoints = './checkpoints/'\n",
    "args.bucket_size = 4\n",
    "args.n_hashes = 4\n",
    "args.is_trainging = True\n",
    "args.root_path = './SYNTHDataset'\n",
    "args.data_path ='SYNTHh1.csv' \n",
    "args.model_id='Synth1_96_24'\n",
    "args.model = 'Autoformer'\n",
    "args.data = 'Synth1'\n",
    "args.features = 'S' #univariate\n",
    "args.seq_len = 96\n",
    "args.label_len = 48\n",
    "args.pred_len = 24\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.n_heads = 8\n",
    "args.factor = 1\n",
    "args.enc_in = 1\n",
    "args.dec_in =1\n",
    "args.c_out = 1\n",
    "args.d_model = 512\n",
    "args.des = 'Exp'\n",
    "args.itr = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 1\n",
    "args.distil = True\n",
    "args.output_attention = False\n",
    "args.patience= 3\n",
    "args.learning_rate = 0.0001\n",
    "args.batch_size = 32 # batch size 32 as in the paper\n",
    "args.embed = 'timeF'\n",
    "args.activation = 'gelu'\n",
    "args.use_amp = False\n",
    "args.loss = 'mse'\n",
    "args.train_epochs = 10 # epoch size is 10 as in the paper\n",
    "\n",
    "\n",
    "# GPU \n",
    "args.gpu = 0\n",
    "args.lradj = 'type1'\n",
    "args.devices = '0,1,2,3'\n",
    "args.use_multi_gpu = False\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "     args.devices = args.devices.replace(' ', '')\n",
    "     device_ids = args.devices.split(',')\n",
    "     args.device_ids = [int(id_) for id_ in device_ids]\n",
    "     args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "1\n",
      ">>>>>>>start training : Synth1_96_24_Autoformer_Synth1_ftS_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.3143713\n",
      "\tspeed: 0.0443s/iter; left time: 113.4498s\n",
      "\titers: 200, epoch: 1 | loss: 0.1954803\n",
      "\tspeed: 0.0360s/iter; left time: 88.5610s\n",
      "Epoch: 1 cost time: 10.460726022720337\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.3389484 Vali Loss: 0.1933758 Test Loss: 0.1858957\n",
      "Validation loss decreased (inf --> 0.193376).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1122024\n",
      "\tspeed: 0.1666s/iter; left time: 382.2853s\n",
      "\titers: 200, epoch: 2 | loss: 0.0861645\n",
      "\tspeed: 0.0416s/iter; left time: 91.2994s\n",
      "Epoch: 2 cost time: 11.356830358505249\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.1275285 Vali Loss: 0.1341929 Test Loss: 0.1322345\n",
      "Validation loss decreased (0.193376 --> 0.134193).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0844183\n",
      "\tspeed: 0.1673s/iter; left time: 339.3827s\n",
      "\titers: 200, epoch: 3 | loss: 0.0671417\n",
      "\tspeed: 0.0368s/iter; left time: 71.0157s\n",
      "Epoch: 3 cost time: 10.192337274551392\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.0804462 Vali Loss: 0.0900373 Test Loss: 0.0931694\n",
      "Validation loss decreased (0.134193 --> 0.090037).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0568757\n",
      "\tspeed: 0.1617s/iter; left time: 285.1129s\n",
      "\titers: 200, epoch: 4 | loss: 0.0622148\n",
      "\tspeed: 0.0381s/iter; left time: 63.3628s\n",
      "Epoch: 4 cost time: 10.284845113754272\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.0683791 Vali Loss: 0.0837477 Test Loss: 0.0864652\n",
      "Validation loss decreased (0.090037 --> 0.083748).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0667637\n",
      "\tspeed: 0.1550s/iter; left time: 231.9962s\n",
      "\titers: 200, epoch: 5 | loss: 0.0433702\n",
      "\tspeed: 0.0371s/iter; left time: 51.8229s\n",
      "Epoch: 5 cost time: 10.201862335205078\n",
      "Epoch: 5, Steps: 266 | Train Loss: 0.0621262 Vali Loss: 0.0773737 Test Loss: 0.0792040\n",
      "Validation loss decreased (0.083748 --> 0.077374).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0478927\n",
      "\tspeed: 0.1439s/iter; left time: 177.0947s\n",
      "\titers: 200, epoch: 6 | loss: 0.0503648\n",
      "\tspeed: 0.0324s/iter; left time: 36.6077s\n",
      "Epoch: 6 cost time: 8.801358222961426\n",
      "Epoch: 6, Steps: 266 | Train Loss: 0.0590888 Vali Loss: 0.0813580 Test Loss: 0.0826665\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0494686\n",
      "\tspeed: 0.1297s/iter; left time: 125.1905s\n",
      "\titers: 200, epoch: 7 | loss: 0.0540701\n",
      "\tspeed: 0.0306s/iter; left time: 26.4262s\n",
      "Epoch: 7 cost time: 8.626061201095581\n",
      "Epoch: 7, Steps: 266 | Train Loss: 0.0573394 Vali Loss: 0.0778691 Test Loss: 0.0805909\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0548705\n",
      "\tspeed: 0.1275s/iter; left time: 89.1106s\n",
      "\titers: 200, epoch: 8 | loss: 0.0542591\n",
      "\tspeed: 0.0343s/iter; left time: 20.5428s\n",
      "Epoch: 8 cost time: 9.05208158493042\n",
      "Epoch: 8, Steps: 266 | Train Loss: 0.0582919 Vali Loss: 0.0770951 Test Loss: 0.0790859\n",
      "Validation loss decreased (0.077374 --> 0.077095).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.0672817\n",
      "\tspeed: 0.1410s/iter; left time: 61.0506s\n",
      "\titers: 200, epoch: 9 | loss: 0.0505394\n",
      "\tspeed: 0.0327s/iter; left time: 10.9052s\n",
      "Epoch: 9 cost time: 9.173526525497437\n",
      "Epoch: 9, Steps: 266 | Train Loss: 0.0611881 Vali Loss: 0.0779511 Test Loss: 0.0798641\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0587492\n",
      "\tspeed: 0.1401s/iter; left time: 23.3893s\n",
      "\titers: 200, epoch: 10 | loss: 0.0535184\n",
      "\tspeed: 0.0371s/iter; left time: 2.4890s\n",
      "Epoch: 10 cost time: 10.064665079116821\n",
      "Epoch: 10, Steps: 266 | Train Loss: 0.0609420 Vali Loss: 0.0765920 Test Loss: 0.0784507\n",
      "Validation loss decreased (0.077095 --> 0.076592).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "2\n",
      ">>>>>>>testing : Synth1_96_24_Autoformer_Synth1_ftS_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2857\n",
      "test shape: (2857, 24, 1) (2857, 24, 1)\n",
      "test shape: (2857, 24, 1) (2857, 24, 1)\n",
      "mse:0.0788557305932045, mae:0.2220548838376999\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp_synth24 = Exp(args)  # set experiments\n",
    "    print(1)\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp_synth24.train(setting)\n",
    "    print(2)\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    mae_synth24 , mse_synth24 , y_pred_synth24, y_true_synth24, first_batch_test_synth24 = exp_synth24.test(setting) \n",
    "    torch.cuda.empty_cache()\n",
    "    print(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2857, 24, 1) (2857, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "#Check the shape of the predictions and compare to real observations\n",
    "print (y_pred_synth24.shape, y_true_synth24.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2857 sequences of length 24, which is the prediction length we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96, 1]) (32, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "#Shape of the the encoder and of the output in a single batch\n",
    "print (first_batch_test_synth24['batch_x'].shape , first_batch_test_synth24['batch_y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96, 4]) torch.Size([32, 72, 4])\n"
     ]
    }
   ],
   "source": [
    "#Shape of the marked data encoder and of the output in a single batch. \n",
    "#Each observation is assigned 4 marks - minute,hour,day of the week and day of the year.\n",
    "print (first_batch_test_synth24['batch_x_mark'].shape , first_batch_test_synth24['batch_y_mark'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second experiment - prediction length 168\n",
    "Set arguements for our model :<br>\n",
    "Data - Synthetic dataset 1<br>\n",
    "Model - Autoformer<br>\n",
    "Frequency - hourly<br>\n",
    "Features - univariate\n",
    "Encoder length - 96<br>\n",
    "Label length - 48<br>\n",
    "Prediction horizon - 168<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All we have to do is change the prediction length arguement, all other arguements stay the same\n",
    "args.pred_len = 168\n",
    "args.model_id='Synth1_96_168'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "1\n",
      ">>>>>>>start training : Synth1_96_168_Autoformer_Synth1_ftS_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8377\n",
      "val 2713\n",
      "test 2713\n",
      "\titers: 100, epoch: 1 | loss: 0.8597628\n",
      "\tspeed: 0.0522s/iter; left time: 131.0589s\n",
      "\titers: 200, epoch: 1 | loss: 0.7179357\n",
      "\tspeed: 0.0495s/iter; left time: 119.2551s\n",
      "Epoch: 1 cost time: 13.303253889083862\n",
      "Epoch: 1, Steps: 261 | Train Loss: 0.7818099 Vali Loss: 0.7682859 Test Loss: 0.8063470\n",
      "Validation loss decreased (inf --> 0.768286).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.4742875\n",
      "\tspeed: 0.1815s/iter; left time: 408.2964s\n",
      "\titers: 200, epoch: 2 | loss: 0.4865830\n",
      "\tspeed: 0.0490s/iter; left time: 105.3118s\n",
      "Epoch: 2 cost time: 12.991724252700806\n",
      "Epoch: 2, Steps: 261 | Train Loss: 0.4661411 Vali Loss: 0.6975989 Test Loss: 0.7464719\n",
      "Validation loss decreased (0.768286 --> 0.697599).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.4524880\n",
      "\tspeed: 0.1645s/iter; left time: 327.1248s\n",
      "\titers: 200, epoch: 3 | loss: 0.3813228\n",
      "\tspeed: 0.0474s/iter; left time: 89.4879s\n",
      "Epoch: 3 cost time: 12.637303829193115\n",
      "Epoch: 3, Steps: 261 | Train Loss: 0.4085672 Vali Loss: 0.6949628 Test Loss: 0.7661759\n",
      "Validation loss decreased (0.697599 --> 0.694963).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.3839853\n",
      "\tspeed: 0.1611s/iter; left time: 278.3940s\n",
      "\titers: 200, epoch: 4 | loss: 0.3137793\n",
      "\tspeed: 0.0477s/iter; left time: 77.5855s\n",
      "Epoch: 4 cost time: 12.721606969833374\n",
      "Epoch: 4, Steps: 261 | Train Loss: 0.3786852 Vali Loss: 0.7465640 Test Loss: 0.8331593\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4194357\n",
      "\tspeed: 0.1621s/iter; left time: 237.8717s\n",
      "\titers: 200, epoch: 5 | loss: 0.4180251\n",
      "\tspeed: 0.0481s/iter; left time: 65.7837s\n",
      "Epoch: 5 cost time: 12.773940563201904\n",
      "Epoch: 5, Steps: 261 | Train Loss: 0.3675106 Vali Loss: 0.7572979 Test Loss: 0.8344876\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.4030178\n",
      "\tspeed: 0.1686s/iter; left time: 203.3796s\n",
      "\titers: 200, epoch: 6 | loss: 0.4221593\n",
      "\tspeed: 0.0459s/iter; left time: 50.7388s\n",
      "Epoch: 6 cost time: 12.67307996749878\n",
      "Epoch: 6, Steps: 261 | Train Loss: 0.3610694 Vali Loss: 0.7558755 Test Loss: 0.8439695\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "2\n",
      ">>>>>>>testing : Synth1_96_168_Autoformer_Synth1_ftS_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2713\n",
      "test shape: (2713, 168, 1) (2713, 168, 1)\n",
      "test shape: (2713, 168, 1) (2713, 168, 1)\n",
      "mse:0.7669336795806885, mae:0.7244773507118225\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp_synth168 = Exp(args)  # set experiments\n",
    "    print(1)\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp_synth168.train(setting)\n",
    "    print(2)\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    mae_synth168 , mse_synth168 , y_pred_synth168, y_true_synth168, first_batch_test_synth168 = exp_synth168.test(setting) \n",
    "    torch.cuda.empty_cache()\n",
    "    print(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third experiment - prediction length 720\n",
    "Set arguements for our model :<br>\n",
    "Data - Synthetic dataset 1<br>\n",
    "Model - Autoformer<br>\n",
    "Frequency - hourly<br>\n",
    "Features - univariate\n",
    "Encoder length - 96<br>\n",
    "Label length - 48<br>\n",
    "Prediction horizon - 720<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pred_len = 720\n",
    "args.model_id='Synth1_96_720'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "1\n",
      ">>>>>>>start training : Synth1_96_720_Autoformer_Synth1_ftS_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7825\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.9376960\n",
      "\tspeed: 0.1235s/iter; left time: 289.1599s\n",
      "\titers: 200, epoch: 1 | loss: 0.6360489\n",
      "\tspeed: 0.1220s/iter; left time: 273.3708s\n",
      "Epoch: 1 cost time: 29.985873222351074\n",
      "Epoch: 1, Steps: 244 | Train Loss: 0.8518100 Vali Loss: 0.9108309 Test Loss: 1.1545044\n",
      "Validation loss decreased (inf --> 0.910831).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.7372466\n",
      "\tspeed: 0.3158s/iter; left time: 662.2377s\n",
      "\titers: 200, epoch: 2 | loss: 0.5897607\n",
      "\tspeed: 0.1139s/iter; left time: 227.4784s\n",
      "Epoch: 2 cost time: 28.314762353897095\n",
      "Epoch: 2, Steps: 244 | Train Loss: 0.6596435 Vali Loss: 0.8983436 Test Loss: 1.1180611\n",
      "Validation loss decreased (0.910831 --> 0.898344).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.6548238\n",
      "\tspeed: 0.3643s/iter; left time: 674.9957s\n",
      "\titers: 200, epoch: 3 | loss: 0.6569330\n",
      "\tspeed: 0.1189s/iter; left time: 208.4123s\n",
      "Epoch: 3 cost time: 29.371445655822754\n",
      "Epoch: 3, Steps: 244 | Train Loss: 0.6290464 Vali Loss: 1.0137494 Test Loss: 1.1894519\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.7097988\n",
      "\tspeed: 0.3072s/iter; left time: 494.2266s\n",
      "\titers: 200, epoch: 4 | loss: 0.5558582\n",
      "\tspeed: 0.1307s/iter; left time: 197.2360s\n",
      "Epoch: 4 cost time: 30.818735361099243\n",
      "Epoch: 4, Steps: 244 | Train Loss: 0.6159579 Vali Loss: 0.9266074 Test Loss: 1.1314818\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.5293211\n",
      "\tspeed: 0.3353s/iter; left time: 457.6272s\n",
      "\titers: 200, epoch: 5 | loss: 0.6276892\n",
      "\tspeed: 0.1239s/iter; left time: 156.7564s\n",
      "Epoch: 5 cost time: 30.281243085861206\n",
      "Epoch: 5, Steps: 244 | Train Loss: 0.6016396 Vali Loss: 1.0980077 Test Loss: 1.1882067\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "2\n",
      ">>>>>>>testing : Synth1_96_720_Autoformer_Synth1_ftS_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2161\n",
      "test shape: (2161, 720, 1) (2161, 720, 1)\n",
      "test shape: (2161, 720, 1) (2161, 720, 1)\n",
      "mse:1.112317442893982, mae:0.8530898690223694\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp_synth720 = Exp(args)  # set experiments\n",
    "    print(1)\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp_synth720.train(setting)\n",
    "    print(2)\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    mae_synth720 , mse_synth720 , y_pred_synth720, y_true_synth720, first_batch_test_synth720 = exp_synth720.test(setting) \n",
    "    torch.cuda.empty_cache()\n",
    "    print(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set arguements for our model :<br>\n",
    "Data - Wind dataset - Germany data<br>\n",
    "Model - Autoformer<br>\n",
    "Frequency - hourly<br>\n",
    "Features - univariate<br>\n",
    "Encoder length - 96<br>\n",
    "Label length - 48<br>\n",
    "Prediction horizon - 24<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.root_path = './WINDataset'\n",
    "args.data_path ='DEWINDh_small.csv' \n",
    "args.model_id='Windh1_96_24'\n",
    "args.data = 'Windh1'\n",
    "args.model = 'Autoformer'\n",
    "args.freq = 'h'\n",
    "args.features = 'S' #univariate\n",
    "args.seq_len = 96\n",
    "args.label_len = 48\n",
    "args.pred_len = 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "1\n",
      ">>>>>>>start training : Windh1_96_24_Autoformer_Windh1_ftS_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.2013479\n",
      "\tspeed: 0.0354s/iter; left time: 90.7487s\n",
      "\titers: 200, epoch: 1 | loss: 0.1326992\n",
      "\tspeed: 0.0335s/iter; left time: 82.4593s\n",
      "Epoch: 1 cost time: 9.12367057800293\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.2505927 Vali Loss: 0.2473148 Test Loss: 0.2902733\n",
      "Validation loss decreased (inf --> 0.247315).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1633435\n",
      "\tspeed: 0.1370s/iter; left time: 314.3430s\n",
      "\titers: 200, epoch: 2 | loss: 0.1217971\n",
      "\tspeed: 0.0317s/iter; left time: 69.6385s\n",
      "Epoch: 2 cost time: 8.699025630950928\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.1608405 Vali Loss: 0.2154517 Test Loss: 0.2235872\n",
      "Validation loss decreased (0.247315 --> 0.215452).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1071070\n",
      "\tspeed: 0.1228s/iter; left time: 249.2460s\n",
      "\titers: 200, epoch: 3 | loss: 0.1674366\n",
      "\tspeed: 0.0310s/iter; left time: 59.7834s\n",
      "Epoch: 3 cost time: 8.51937198638916\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.1424458 Vali Loss: 0.2059410 Test Loss: 0.1920224\n",
      "Validation loss decreased (0.215452 --> 0.205941).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1175552\n",
      "\tspeed: 0.1293s/iter; left time: 227.9575s\n",
      "\titers: 200, epoch: 4 | loss: 0.2154239\n",
      "\tspeed: 0.0354s/iter; left time: 58.8239s\n",
      "Epoch: 4 cost time: 9.250093460083008\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.1351373 Vali Loss: 0.2023038 Test Loss: 0.1823626\n",
      "Validation loss decreased (0.205941 --> 0.202304).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1242567\n",
      "\tspeed: 0.1410s/iter; left time: 211.0503s\n",
      "\titers: 200, epoch: 5 | loss: 0.1379724\n",
      "\tspeed: 0.0343s/iter; left time: 47.9777s\n",
      "Epoch: 5 cost time: 9.48917555809021\n",
      "Epoch: 5, Steps: 266 | Train Loss: 0.1272269 Vali Loss: 0.2064259 Test Loss: 0.2100024\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1296379\n",
      "\tspeed: 0.1341s/iter; left time: 165.0436s\n",
      "\titers: 200, epoch: 6 | loss: 0.1156319\n",
      "\tspeed: 0.0310s/iter; left time: 35.0413s\n",
      "Epoch: 6 cost time: 8.538236141204834\n",
      "Epoch: 6, Steps: 266 | Train Loss: 0.1256406 Vali Loss: 0.2069827 Test Loss: 0.1871945\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1371730\n",
      "\tspeed: 0.1258s/iter; left time: 121.3878s\n",
      "\titers: 200, epoch: 7 | loss: 0.1164179\n",
      "\tspeed: 0.0337s/iter; left time: 29.1623s\n",
      "Epoch: 7 cost time: 9.16286849975586\n",
      "Epoch: 7, Steps: 266 | Train Loss: 0.1242134 Vali Loss: 0.2148817 Test Loss: 0.1951957\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "2\n",
      ">>>>>>>testing : Windh1_96_24_Autoformer_Windh1_ftS_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2857\n",
      "test shape: (2857, 24, 1) (2857, 24, 1)\n",
      "test shape: (2857, 24, 1) (2857, 24, 1)\n",
      "mse:0.18275713920593262, mae:0.28731775283813477\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp_wind24 = Exp(args)  # set experiments\n",
    "    print(1)\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp_wind24.train(setting)\n",
    "    print(2)\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    mae_wind24 , mse_wind24 , y_pred_wind24, y_true_wind24, first_batch_test_wind24 = exp_wind24.test(setting) \n",
    "    torch.cuda.empty_cache()\n",
    "    print(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second experiment - prediction length 168\n",
    "Set arguements for our model :<br>\n",
    "Data - Wind dataset 1<br>\n",
    "Model - Autoformer<br>\n",
    "Frequency - hourly<br>\n",
    "Feature - univariate<br>\n",
    "Encoder length - 96<br>\n",
    "Label length - 48<br>\n",
    "Prediction horizon - 168<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_id='Windh1_96_168'\n",
    "args.pred_len = 168\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "1\n",
      ">>>>>>>start training : Windh1_96_168_Autoformer_Windh1_ftS_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8377\n",
      "val 2713\n",
      "test 2713\n",
      "\titers: 100, epoch: 1 | loss: 0.2818532\n",
      "\tspeed: 0.0506s/iter; left time: 126.9818s\n",
      "\titers: 200, epoch: 1 | loss: 0.2483424\n",
      "\tspeed: 0.0481s/iter; left time: 115.9414s\n",
      "Epoch: 1 cost time: 12.970085620880127\n",
      "Epoch: 1, Steps: 261 | Train Loss: 0.3334505 Vali Loss: 0.2943013 Test Loss: 0.2996873\n",
      "Validation loss decreased (inf --> 0.294301).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2628716\n",
      "\tspeed: 0.2056s/iter; left time: 462.6881s\n",
      "\titers: 200, epoch: 2 | loss: 0.2037293\n",
      "\tspeed: 0.0514s/iter; left time: 110.6053s\n",
      "Epoch: 2 cost time: 13.780773401260376\n",
      "Epoch: 2, Steps: 261 | Train Loss: 0.2522194 Vali Loss: 0.2668315 Test Loss: 0.2879464\n",
      "Validation loss decreased (0.294301 --> 0.266832).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2421365\n",
      "\tspeed: 0.1790s/iter; left time: 355.9947s\n",
      "\titers: 200, epoch: 3 | loss: 0.2216545\n",
      "\tspeed: 0.0500s/iter; left time: 94.4321s\n",
      "Epoch: 3 cost time: 13.329171895980835\n",
      "Epoch: 3, Steps: 261 | Train Loss: 0.2315376 Vali Loss: 0.2572832 Test Loss: 0.2738938\n",
      "Validation loss decreased (0.266832 --> 0.257283).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2332806\n",
      "\tspeed: 0.1710s/iter; left time: 295.4315s\n",
      "\titers: 200, epoch: 4 | loss: 0.1932527\n",
      "\tspeed: 0.0462s/iter; left time: 75.2499s\n",
      "Epoch: 4 cost time: 12.409553289413452\n",
      "Epoch: 4, Steps: 261 | Train Loss: 0.2237839 Vali Loss: 0.2569453 Test Loss: 0.2806977\n",
      "Validation loss decreased (0.257283 --> 0.256945).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1917441\n",
      "\tspeed: 0.1731s/iter; left time: 253.9561s\n",
      "\titers: 200, epoch: 5 | loss: 0.2146890\n",
      "\tspeed: 0.0494s/iter; left time: 67.5294s\n",
      "Epoch: 5 cost time: 13.329760789871216\n",
      "Epoch: 5, Steps: 261 | Train Loss: 0.2203207 Vali Loss: 0.2657641 Test Loss: 0.2587827\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.2361454\n",
      "\tspeed: 0.1787s/iter; left time: 215.4979s\n",
      "\titers: 200, epoch: 6 | loss: 0.2159083\n",
      "\tspeed: 0.0467s/iter; left time: 51.6801s\n",
      "Epoch: 6 cost time: 12.74780011177063\n",
      "Epoch: 6, Steps: 261 | Train Loss: 0.2181305 Vali Loss: 0.2539068 Test Loss: 0.2660047\n",
      "Validation loss decreased (0.256945 --> 0.253907).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.2610320\n",
      "\tspeed: 0.1791s/iter; left time: 169.2407s\n",
      "\titers: 200, epoch: 7 | loss: 0.2304826\n",
      "\tspeed: 0.0476s/iter; left time: 40.2309s\n",
      "Epoch: 7 cost time: 12.791812896728516\n",
      "Epoch: 7, Steps: 261 | Train Loss: 0.2169784 Vali Loss: 0.2523196 Test Loss: 0.2658391\n",
      "Validation loss decreased (0.253907 --> 0.252320).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2162459\n",
      "\tspeed: 0.1655s/iter; left time: 113.2331s\n",
      "\titers: 200, epoch: 8 | loss: 0.2314133\n",
      "\tspeed: 0.0486s/iter; left time: 28.3823s\n",
      "Epoch: 8 cost time: 12.826024293899536\n",
      "Epoch: 8, Steps: 261 | Train Loss: 0.2166320 Vali Loss: 0.2517700 Test Loss: 0.2626263\n",
      "Validation loss decreased (0.252320 --> 0.251770).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1966455\n",
      "\tspeed: 0.1583s/iter; left time: 66.9791s\n",
      "\titers: 200, epoch: 9 | loss: 0.2021026\n",
      "\tspeed: 0.0462s/iter; left time: 14.9376s\n",
      "Epoch: 9 cost time: 12.474206924438477\n",
      "Epoch: 9, Steps: 261 | Train Loss: 0.2161060 Vali Loss: 0.2519391 Test Loss: 0.2646679\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.2214966\n",
      "\tspeed: 0.1849s/iter; left time: 29.9523s\n",
      "\titers: 200, epoch: 10 | loss: 0.2433699\n",
      "\tspeed: 0.0498s/iter; left time: 3.0885s\n",
      "Epoch: 10 cost time: 13.297359704971313\n",
      "Epoch: 10, Steps: 261 | Train Loss: 0.2159361 Vali Loss: 0.2514585 Test Loss: 0.2631235\n",
      "Validation loss decreased (0.251770 --> 0.251458).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "2\n",
      ">>>>>>>testing : Windh1_96_168_Autoformer_Windh1_ftS_sl96_ll48_pl168_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2713\n",
      "test shape: (2713, 168, 1) (2713, 168, 1)\n",
      "test shape: (2713, 168, 1) (2713, 168, 1)\n",
      "mse:0.26259592175483704, mae:0.35298752784729004\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp_wind168 = Exp(args)  # set experiments\n",
    "    print(1)\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp_wind168.train(setting)\n",
    "    print(2)\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    mae_wind168 , mse_wind168 , y_pred_wind168, y_true_wind168, first_batch_test_wind168 = exp_wind168.test(setting) \n",
    "    torch.cuda.empty_cache()\n",
    "    print(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third experiment - prediction length 720\n",
    "Set arguements for our model :<br>\n",
    "Data - Wind dataset 1<br>\n",
    "Model - Autoformer<br>\n",
    "Frequency - hourly<br>\n",
    "Feature - univariate<br>\n",
    "Encoder length - 96<br>\n",
    "Label length - 48<br>\n",
    "Prediction horizon - 720<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_id='Windh1_96_720'\n",
    "args.pred_len = 720\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "1\n",
      ">>>>>>>start training : Windh1_96_720_Autoformer_Windh1_ftS_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7825\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.3441183\n",
      "\tspeed: 0.1244s/iter; left time: 291.3119s\n",
      "\titers: 200, epoch: 1 | loss: 0.3367561\n",
      "\tspeed: 0.1188s/iter; left time: 266.2574s\n",
      "Epoch: 1 cost time: 29.576159238815308\n",
      "Epoch: 1, Steps: 244 | Train Loss: 0.3685992 Vali Loss: 0.3536319 Test Loss: 0.3061326\n",
      "Validation loss decreased (inf --> 0.353632).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3035700\n",
      "\tspeed: 0.3125s/iter; left time: 655.2985s\n",
      "\titers: 200, epoch: 2 | loss: 0.2501293\n",
      "\tspeed: 0.1279s/iter; left time: 255.3315s\n",
      "Epoch: 2 cost time: 30.792705297470093\n",
      "Epoch: 2, Steps: 244 | Train Loss: 0.2886758 Vali Loss: 0.3844312 Test Loss: 0.2870901\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.2599574\n",
      "\tspeed: 0.3346s/iter; left time: 620.0090s\n",
      "\titers: 200, epoch: 3 | loss: 0.2853165\n",
      "\tspeed: 0.1219s/iter; left time: 213.6764s\n",
      "Epoch: 3 cost time: 29.912846326828003\n",
      "Epoch: 3, Steps: 244 | Train Loss: 0.2612352 Vali Loss: 0.4147673 Test Loss: 0.2718137\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.2352184\n",
      "\tspeed: 0.3533s/iter; left time: 568.4425s\n",
      "\titers: 200, epoch: 4 | loss: 0.2554759\n",
      "\tspeed: 0.1177s/iter; left time: 177.6492s\n",
      "Epoch: 4 cost time: 30.623806715011597\n",
      "Epoch: 4, Steps: 244 | Train Loss: 0.2460559 Vali Loss: 0.4249613 Test Loss: 0.2617814\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "2\n",
      ">>>>>>>testing : Windh1_96_720_Autoformer_Windh1_ftS_sl96_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2161\n",
      "test shape: (2161, 720, 1) (2161, 720, 1)\n",
      "test shape: (2161, 720, 1) (2161, 720, 1)\n",
      "mse:0.3058130741119385, mae:0.40483126044273376\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des, ii)\n",
    "\n",
    "    exp_wind720 = Exp(args)  # set experiments\n",
    "    print(1)\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp_wind720.train(setting)\n",
    "    print(2)\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    mae_wind720 , mse_wind720 , y_pred_wind720, y_true_wind720, first_batch_test_wind720 = exp_wind720.test(setting) \n",
    "    torch.cuda.empty_cache()\n",
    "    print(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
