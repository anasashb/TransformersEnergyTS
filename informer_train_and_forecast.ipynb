{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informer Forecasting Notebook\n",
    "\n",
    "In the following notebook we will train the Informer on the synthetic and German wind generation data for three forecasting scenarios with prediction windows of 24, 168 and 720 hours.\n",
    "\n",
    "To optimize the model training and forecasting experience for Jupyter Notebook, minor changes were made to `Informer2020/exp/exp_informer.py`. To allow for using the synthetic and wind datasets further amendments were made to `Informer2020/exp/exp_informer.py` and `Informer2020/data/data_loader.py`.\n",
    "\n",
    "For the initial prototype, all models are trained for 8 epochs as in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# Add Informer to path\n",
    "import sys\n",
    "if not 'Informer2020' in sys.path:\n",
    "    sys.path += ['Informer2020']\n",
    "# Import informer and argument parser\n",
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Synthetic Data Forecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1) 24-Hour Windows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a long list of model arguments, much of which will be reusable for further forecasting experiments.\n",
    "\n",
    "**For ease of replication:** `args.data`, `args.root_path` and `args.data_path` can be changed to train and predict on a different dataset, and `args.pred_len` can be changed to switch to a different prediction window. Rest of the specifications are defined in accordance with the experiments of the original paper that yielded best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informerstack' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "# Use synthetic data\n",
    "args.data = 'SYNTHh1' # data\n",
    "args.root_path = './SYNTHDataset/' # root path of data file\n",
    "args.data_path = 'SYNTHh1.csv' # data file\n",
    "# Set up univariate forecasting\n",
    "args.features = 'S' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'TARGET' # target feature in S or MS task\n",
    "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 96 # input sequence length of Informer encoder\n",
    "args.label_len = 48 # start token length of Informer decoder\n",
    "args.pred_len = 24 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "# Architecture specifics\n",
    "args.enc_in = 1 # encoder input size\n",
    "args.dec_in = 1 # decoder input size\n",
    "args.c_out = 1 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.s_layers = [3, 2, 1] # num of encoder layers\n",
    "args.d_layers = 2 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'h'\n",
    "\n",
    "args.batch_size = 32 \n",
    "args.learning_rate = 1e-4\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 8\n",
    "args.patience = 3\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n",
    "\n",
    "# GPU Handling\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "# Data parser\n",
    "# Set augments by using data name\n",
    "data_parser = {\n",
    "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'SYNTHh1':{'data':'SYNTHh1.csv','T':'TARGET','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]}, ## our new dataset\n",
    "    'SYNTHh2':{'data':'SYNTHh2.csv','T':'TARGET','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]}, ## our new dataset\n",
    "    'DEWINDh_large':{'data':'DEWINDh_large.csv','T':'TARGET','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]}, ## our new dataset\n",
    "    'DEWINDh_small':{'data':'DEWINDh_small.csv','T':'TARGET','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]}, ## our new dataset\n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]\n",
    "\n",
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the experiment specifications can be inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'SYNTHh1', 'root_path': './SYNTHDataset/', 'data_path': 'SYNTHh1.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize informer\n",
    "Exp_synth_24 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_SYNTHh1_ftS_sl96_ll48_pl24_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.1211558\n",
      "\tspeed: 0.1539s/iter; left time: 312.3286s\n",
      "\titers: 200, epoch: 1 | loss: 0.0378935\n",
      "\tspeed: 0.1457s/iter; left time: 281.0465s\n",
      "Epoch: 1 cost time: 39.40832161903381\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.1696828 Vali Loss: 0.0457727 Test Loss: 0.0442011\n",
      "Validation loss decreased (inf --> 0.045773).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0371760\n",
      "\tspeed: 0.3366s/iter; left time: 593.5103s\n",
      "\titers: 200, epoch: 2 | loss: 0.0327412\n",
      "\tspeed: 0.1444s/iter; left time: 240.1888s\n",
      "Epoch: 2 cost time: 38.64145588874817\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.0387668 Vali Loss: 0.0467871 Test Loss: 0.0459842\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0370206\n",
      "\tspeed: 0.3023s/iter; left time: 452.4817s\n",
      "\titers: 200, epoch: 3 | loss: 0.0251893\n",
      "\tspeed: 0.1456s/iter; left time: 203.4149s\n",
      "Epoch: 3 cost time: 37.43153405189514\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.0266075 Vali Loss: 0.0292618 Test Loss: 0.0297471\n",
      "Validation loss decreased (0.045773 --> 0.029262).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0202201\n",
      "\tspeed: 0.3408s/iter; left time: 419.4884s\n",
      "\titers: 200, epoch: 4 | loss: 0.0231499\n",
      "\tspeed: 0.1474s/iter; left time: 166.7503s\n",
      "Epoch: 4 cost time: 39.19699954986572\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.0225791 Vali Loss: 0.0323898 Test Loss: 0.0320516\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0194972\n",
      "\tspeed: 0.3388s/iter; left time: 326.8996s\n",
      "\titers: 200, epoch: 5 | loss: 0.0214950\n",
      "\tspeed: 0.1482s/iter; left time: 128.1817s\n",
      "Epoch: 5 cost time: 38.45511817932129\n",
      "Epoch: 5, Steps: 266 | Train Loss: 0.0213005 Vali Loss: 0.0307097 Test Loss: 0.0311951\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0262282\n",
      "\tspeed: 0.3024s/iter; left time: 211.3484s\n",
      "\titers: 200, epoch: 6 | loss: 0.0190445\n",
      "\tspeed: 0.1475s/iter; left time: 88.3441s\n",
      "Epoch: 6 cost time: 39.176613330841064\n",
      "Epoch: 6, Steps: 266 | Train Loss: 0.0204968 Vali Loss: 0.0253789 Test Loss: 0.0263420\n",
      "Validation loss decreased (0.029262 --> 0.025379).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0171007\n",
      "\tspeed: 0.3410s/iter; left time: 147.6529s\n",
      "\titers: 200, epoch: 7 | loss: 0.0197117\n",
      "\tspeed: 0.1485s/iter; left time: 49.4411s\n",
      "Epoch: 7 cost time: 39.39856576919556\n",
      "Epoch: 7, Steps: 266 | Train Loss: 0.0199023 Vali Loss: 0.0240748 Test Loss: 0.0249990\n",
      "Validation loss decreased (0.025379 --> 0.024075).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0177807\n",
      "\tspeed: 0.3431s/iter; left time: 57.2916s\n",
      "\titers: 200, epoch: 8 | loss: 0.0177348\n",
      "\tspeed: 0.1475s/iter; left time: 9.8823s\n",
      "Epoch: 8 cost time: 36.485780239105225\n",
      "Epoch: 8, Steps: 266 | Train Loss: 0.0197047 Vali Loss: 0.0248656 Test Loss: 0.0255376\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      ">>>>>>>testing : informerstack_SYNTHh1_ftS_sl96_ll48_pl24_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2857\n",
      "test shape: (89, 32, 24, 1) (89, 32, 24, 1)\n",
      "test shape: (2848, 24, 1) (2848, 24, 1)\n",
      "mse:0.024920832365751266, mae:0.1255515217781067\n"
     ]
    }
   ],
   "source": [
    "# train and predict 24 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_synth_24 = Exp_synth_24(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_synth_24.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    synth_pred_24, synth_true_24, synth_mse_24, synth_mae_24, synth_24_first_batch_test = exp_synth_24.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2) 168-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'SYNTHh1', 'root_path': './SYNTHDataset/', 'data_path': 'SYNTHh1.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 168, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Increase prediction length\n",
    "args.pred_len = 168 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_synth_168 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_SYNTHh1_ftS_sl96_ll48_pl168_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8377\n",
      "val 2713\n",
      "test 2713\n",
      "\titers: 100, epoch: 1 | loss: 0.2414376\n",
      "\tspeed: 0.2124s/iter; left time: 422.3871s\n",
      "\titers: 200, epoch: 1 | loss: 0.0868091\n",
      "\tspeed: 0.2148s/iter; left time: 405.6950s\n",
      "Epoch: 1 cost time: 55.95195031166077\n",
      "Epoch: 1, Steps: 261 | Train Loss: 0.2622942 Vali Loss: 0.1683587 Test Loss: 0.1533240\n",
      "Validation loss decreased (inf --> 0.168359).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0605847\n",
      "\tspeed: 0.4739s/iter; left time: 818.8904s\n",
      "\titers: 200, epoch: 2 | loss: 0.0458365\n",
      "\tspeed: 0.2149s/iter; left time: 349.8871s\n",
      "Epoch: 2 cost time: 53.554131507873535\n",
      "Epoch: 2, Steps: 261 | Train Loss: 0.0608510 Vali Loss: 0.1112561 Test Loss: 0.1085942\n",
      "Validation loss decreased (0.168359 --> 0.111256).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0473631\n",
      "\tspeed: 0.5027s/iter; left time: 737.4985s\n",
      "\titers: 200, epoch: 3 | loss: 0.0302710\n",
      "\tspeed: 0.2148s/iter; left time: 293.6874s\n",
      "Epoch: 3 cost time: 56.13202691078186\n",
      "Epoch: 3, Steps: 261 | Train Loss: 0.0397620 Vali Loss: 0.0830881 Test Loss: 0.0867405\n",
      "Validation loss decreased (0.111256 --> 0.083088).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0373799\n",
      "\tspeed: 0.4758s/iter; left time: 573.8384s\n",
      "\titers: 200, epoch: 4 | loss: 0.0286217\n",
      "\tspeed: 0.2125s/iter; left time: 235.0637s\n",
      "Epoch: 4 cost time: 53.433438777923584\n",
      "Epoch: 4, Steps: 261 | Train Loss: 0.0335564 Vali Loss: 0.1065375 Test Loss: 0.1097763\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0302864\n",
      "\tspeed: 0.4956s/iter; left time: 468.3162s\n",
      "\titers: 200, epoch: 5 | loss: 0.0295147\n",
      "\tspeed: 0.2148s/iter; left time: 181.4719s\n",
      "Epoch: 5 cost time: 55.87813186645508\n",
      "Epoch: 5, Steps: 261 | Train Loss: 0.0287875 Vali Loss: 0.0838564 Test Loss: 0.0855295\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0272671\n",
      "\tspeed: 0.4756s/iter; left time: 325.3157s\n",
      "\titers: 200, epoch: 6 | loss: 0.0258308\n",
      "\tspeed: 0.2146s/iter; left time: 125.3338s\n",
      "Epoch: 6 cost time: 53.900904178619385\n",
      "Epoch: 6, Steps: 261 | Train Loss: 0.0270610 Vali Loss: 0.1068945 Test Loss: 0.1159694\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : informerstack_SYNTHh1_ftS_sl96_ll48_pl168_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2713\n",
      "test shape: (84, 32, 168, 1) (84, 32, 168, 1)\n",
      "test shape: (2688, 168, 1) (2688, 168, 1)\n",
      "mse:0.08663419634103775, mae:0.23446011543273926\n"
     ]
    }
   ],
   "source": [
    "# Train and predict 168 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_synth_168 = Exp_synth_168(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_synth_168.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    synth_pred_168, synth_true_168, synth_mse_168, synth_mae_168, synth_168_first_batch_test = exp_synth_168.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3) 720-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'SYNTHh1', 'root_path': './SYNTHDataset/', 'data_path': 'SYNTHh1.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 720, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Increase prediction length\n",
    "args.pred_len = 720 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_synth_720 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_SYNTHh1_ftS_sl96_ll48_pl720_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7825\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.3717384\n",
      "\tspeed: 0.3830s/iter; left time: 709.7644s\n"
     ]
    }
   ],
   "source": [
    "# Train and predict 720 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_synth_720 = Exp_synth_720(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_synth_720.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    synth_pred_720, synth_true_720, synth_mse_720, synth_mae_720, synth_720_first_batch_test = exp_synth_720.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Wind Data Forecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1) 24-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data to wind\n",
    "args.data = 'DEWINDh_small' # data\n",
    "args.root_path = './WINDataset/' # root path of data file\n",
    "args.data_path = 'DEWINDh_small.csv' # data file\n",
    "\n",
    "# Re-set prediction length to 24\n",
    "args.pred_len = 24 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_wind_24 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict 24 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_wind_24 = Exp_wind_24(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_wind_24.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    wind_pred_24, wind_true_24, wind_mse_24, wind_mae_24, wind_24_first_batch_test = exp_wind_24.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2) 168-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'SYNTHh1', 'root_path': './SYNTHDataset/', 'data_path': 'SYNTHh1.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 168, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Increase prediction length\n",
    "args.pred_len = 168 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_wind_168 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_SYNTHh1_ftS_sl96_ll48_pl168_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8377\n",
      "val 2713\n",
      "test 2713\n",
      "\titers: 100, epoch: 1 | loss: 0.2414376\n",
      "\tspeed: 0.2124s/iter; left time: 422.3871s\n",
      "\titers: 200, epoch: 1 | loss: 0.0868091\n",
      "\tspeed: 0.2148s/iter; left time: 405.6950s\n",
      "Epoch: 1 cost time: 55.95195031166077\n",
      "Epoch: 1, Steps: 261 | Train Loss: 0.2622942 Vali Loss: 0.1683587 Test Loss: 0.1533240\n",
      "Validation loss decreased (inf --> 0.168359).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0605847\n",
      "\tspeed: 0.4739s/iter; left time: 818.8904s\n",
      "\titers: 200, epoch: 2 | loss: 0.0458365\n",
      "\tspeed: 0.2149s/iter; left time: 349.8871s\n",
      "Epoch: 2 cost time: 53.554131507873535\n",
      "Epoch: 2, Steps: 261 | Train Loss: 0.0608510 Vali Loss: 0.1112561 Test Loss: 0.1085942\n",
      "Validation loss decreased (0.168359 --> 0.111256).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0473631\n",
      "\tspeed: 0.5027s/iter; left time: 737.4985s\n",
      "\titers: 200, epoch: 3 | loss: 0.0302710\n",
      "\tspeed: 0.2148s/iter; left time: 293.6874s\n",
      "Epoch: 3 cost time: 56.13202691078186\n",
      "Epoch: 3, Steps: 261 | Train Loss: 0.0397620 Vali Loss: 0.0830881 Test Loss: 0.0867405\n",
      "Validation loss decreased (0.111256 --> 0.083088).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0373799\n",
      "\tspeed: 0.4758s/iter; left time: 573.8384s\n",
      "\titers: 200, epoch: 4 | loss: 0.0286217\n",
      "\tspeed: 0.2125s/iter; left time: 235.0637s\n",
      "Epoch: 4 cost time: 53.433438777923584\n",
      "Epoch: 4, Steps: 261 | Train Loss: 0.0335564 Vali Loss: 0.1065375 Test Loss: 0.1097763\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0302864\n",
      "\tspeed: 0.4956s/iter; left time: 468.3162s\n",
      "\titers: 200, epoch: 5 | loss: 0.0295147\n",
      "\tspeed: 0.2148s/iter; left time: 181.4719s\n",
      "Epoch: 5 cost time: 55.87813186645508\n",
      "Epoch: 5, Steps: 261 | Train Loss: 0.0287875 Vali Loss: 0.0838564 Test Loss: 0.0855295\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0272671\n",
      "\tspeed: 0.4756s/iter; left time: 325.3157s\n",
      "\titers: 200, epoch: 6 | loss: 0.0258308\n",
      "\tspeed: 0.2146s/iter; left time: 125.3338s\n",
      "Epoch: 6 cost time: 53.900904178619385\n",
      "Epoch: 6, Steps: 261 | Train Loss: 0.0270610 Vali Loss: 0.1068945 Test Loss: 0.1159694\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : informerstack_SYNTHh1_ftS_sl96_ll48_pl168_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2713\n",
      "test shape: (84, 32, 168, 1) (84, 32, 168, 1)\n",
      "test shape: (2688, 168, 1) (2688, 168, 1)\n",
      "mse:0.08663419634103775, mae:0.23446011543273926\n"
     ]
    }
   ],
   "source": [
    "# Train and predict 168 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_wind_168 = Exp_wind_168(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_wind_168.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    wind_pred_168, wind_true_168, wind_mse_168, wind_mae_168, wind_168_first_batch_test = exp_wind_168.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3) 720-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'SYNTHh1', 'root_path': './SYNTHDataset/', 'data_path': 'SYNTHh1.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 720, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Increase prediction length\n",
    "args.pred_len = 720 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_wind_720 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_SYNTHh1_ftS_sl96_ll48_pl720_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7825\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.3430235\n",
      "\tspeed: 0.4018s/iter; left time: 744.4869s\n",
      "\titers: 200, epoch: 1 | loss: 0.3133819\n",
      "\tspeed: 0.4077s/iter; left time: 714.7197s\n",
      "Epoch: 1 cost time: 98.89167380332947\n",
      "Epoch: 1, Steps: 244 | Train Loss: 0.4217238 Vali Loss: 0.4946858 Test Loss: 0.4545954\n",
      "Validation loss decreased (inf --> 0.494686).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2403031\n",
      "\tspeed: 0.8125s/iter; left time: 1307.2512s\n",
      "\titers: 200, epoch: 2 | loss: 0.0864182\n",
      "\tspeed: 0.4077s/iter; left time: 615.2820s\n",
      "Epoch: 2 cost time: 98.66395902633667\n",
      "Epoch: 2, Steps: 244 | Train Loss: 0.1953741 Vali Loss: 0.1766270 Test Loss: 0.1507086\n",
      "Validation loss decreased (0.494686 --> 0.176627).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0415380\n",
      "\tspeed: 0.8193s/iter; left time: 1118.3354s\n",
      "\titers: 200, epoch: 3 | loss: 0.0326844\n",
      "\tspeed: 0.3957s/iter; left time: 500.5451s\n",
      "Epoch: 3 cost time: 97.03030157089233\n",
      "Epoch: 3, Steps: 244 | Train Loss: 0.0406374 Vali Loss: 0.1863890 Test Loss: 0.1560555\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0281307\n",
      "\tspeed: 0.8176s/iter; left time: 916.5703s\n",
      "\titers: 200, epoch: 4 | loss: 0.0304739\n",
      "\tspeed: 0.4051s/iter; left time: 413.5911s\n",
      "Epoch: 4 cost time: 96.84927678108215\n",
      "Epoch: 4, Steps: 244 | Train Loss: 0.0310173 Vali Loss: 0.1306410 Test Loss: 0.1043678\n",
      "Validation loss decreased (0.176627 --> 0.130641).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0266581\n",
      "\tspeed: 0.8073s/iter; left time: 708.0298s\n",
      "\titers: 200, epoch: 5 | loss: 0.0270630\n",
      "\tspeed: 0.4051s/iter; left time: 314.7390s\n",
      "Epoch: 5 cost time: 99.31166481971741\n",
      "Epoch: 5, Steps: 244 | Train Loss: 0.0279001 Vali Loss: 0.1282520 Test Loss: 0.1060942\n",
      "Validation loss decreased (0.130641 --> 0.128252).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0241831\n",
      "\tspeed: 0.8103s/iter; left time: 512.9316s\n",
      "\titers: 200, epoch: 6 | loss: 0.0267989\n",
      "\tspeed: 0.4095s/iter; left time: 218.2696s\n",
      "Epoch: 6 cost time: 97.84300923347473\n",
      "Epoch: 6, Steps: 244 | Train Loss: 0.0264384 Vali Loss: 0.1201355 Test Loss: 0.0973592\n",
      "Validation loss decreased (0.128252 --> 0.120136).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0238170\n",
      "\tspeed: 0.8311s/iter; left time: 323.3034s\n",
      "\titers: 200, epoch: 7 | loss: 0.0296652\n",
      "\tspeed: 0.3907s/iter; left time: 112.9171s\n",
      "Epoch: 7 cost time: 97.70092248916626\n",
      "Epoch: 7, Steps: 244 | Train Loss: 0.0257387 Vali Loss: 0.1231875 Test Loss: 0.0987965\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0244920\n",
      "\tspeed: 0.8234s/iter; left time: 119.3960s\n",
      "\titers: 200, epoch: 8 | loss: 0.0233841\n",
      "\tspeed: 0.4066s/iter; left time: 18.2952s\n",
      "Epoch: 8 cost time: 97.30134224891663\n",
      "Epoch: 8, Steps: 244 | Train Loss: 0.0252586 Vali Loss: 0.1276509 Test Loss: 0.1022036\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      ">>>>>>>testing : informerstack_SYNTHh1_ftS_sl96_ll48_pl720_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2161\n",
      "test shape: (67, 32, 168, 1) (67, 32, 720, 1)\n",
      "test shape: (2144, 168, 1) (2144, 720, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train and predict 720 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_wind_720 = Exp_wind_720(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_wind_720.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    wind_pred_720, wind_true_720, wind_mse_720, wind_mae_720, wind_720_first_batch_test = exp_wind_720.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anbs_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
