{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informer Forecasting Notebook\n",
    "\n",
    "In the following notebook we will train the Informer on the synthetic and German wind generation data for three forecasting scenarios with prediction windows of 24, 168 and 720 hours.\n",
    "\n",
    "To optimize the model training and forecasting experience for Jupyter Notebook, minor changes were made to `Informer2020/exp/exp_informer.py`. To allow for using the synthetic and wind datasets further amendments were made to `Informer2020/exp/exp_informer.py` and `Informer2020/data/data_loader.py`.\n",
    "\n",
    "For the initial prototype, all models are trained for 8 epochs as in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# Add Informer to path\n",
    "import sys\n",
    "if not 'Informer2020' in sys.path:\n",
    "    sys.path += ['Informer2020']\n",
    "# Import informer and argument parser\n",
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Synthetic Data Forecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1) 24-Hour Windows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a long list of model arguments, much of which will be reusable for further forecasting experiments.\n",
    "\n",
    "**For ease of replication:** `args.data`, `args.root_path` and `args.data_path` can be changed to train and predict on a different dataset, and `args.pred_len` can be changed to switch to a different prediction window. Rest of the specifications are defined in accordance with the experiments of the original paper that yielded best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informerstack' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "# Use synthetic data\n",
    "args.data = 'SYNTHh1' # data\n",
    "args.root_path = './SYNTHDataset/' # root path of data file\n",
    "args.data_path = 'SYNTHh1.csv' # data file\n",
    "# Set up univariate forecasting\n",
    "args.features = 'S' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'TARGET' # target feature in S or MS task\n",
    "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 96 # input sequence length of Informer encoder\n",
    "args.label_len = 48 # start token length of Informer decoder\n",
    "args.pred_len = 24 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "# Architecture specifics\n",
    "args.enc_in = 1 # encoder input size\n",
    "args.dec_in = 1 # decoder input size\n",
    "args.c_out = 1 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.s_layers = [3, 2, 1] # num of encoder layers\n",
    "args.d_layers = 2 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'h'\n",
    "\n",
    "args.batch_size = 32 \n",
    "args.learning_rate = 1e-4\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 8\n",
    "args.patience = 3\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n",
    "\n",
    "# GPU Handling\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "# Data parser\n",
    "# Set augments by using data name\n",
    "data_parser = {\n",
    "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
    "    'SYNTHh1':{'data':'SYNTHh1.csv','T':'TARGET','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]}, ## our new dataset\n",
    "    'SYNTHh2':{'data':'SYNTHh2.csv','T':'TARGET','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]}, ## our new dataset\n",
    "    'DEWINDh_large':{'data':'DEWINDh_large.csv','T':'TARGET','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]}, ## our new dataset\n",
    "    'DEWINDh_small':{'data':'DEWINDh_small.csv','T':'TARGET','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]}, ## our new dataset\n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]\n",
    "\n",
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the experiment specifications can be inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'SYNTHh1', 'root_path': './SYNTHDataset/', 'data_path': 'SYNTHh1.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize informer\n",
    "Exp_synth_24 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_SYNTHh1_ftS_sl96_ll48_pl24_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.1211558\n",
      "\tspeed: 0.1539s/iter; left time: 312.3286s\n",
      "\titers: 200, epoch: 1 | loss: 0.0378935\n",
      "\tspeed: 0.1457s/iter; left time: 281.0465s\n",
      "Epoch: 1 cost time: 39.40832161903381\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.1696828 Vali Loss: 0.0457727 Test Loss: 0.0442011\n",
      "Validation loss decreased (inf --> 0.045773).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0371760\n",
      "\tspeed: 0.3366s/iter; left time: 593.5103s\n",
      "\titers: 200, epoch: 2 | loss: 0.0327412\n",
      "\tspeed: 0.1444s/iter; left time: 240.1888s\n",
      "Epoch: 2 cost time: 38.64145588874817\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.0387668 Vali Loss: 0.0467871 Test Loss: 0.0459842\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0370206\n",
      "\tspeed: 0.3023s/iter; left time: 452.4817s\n",
      "\titers: 200, epoch: 3 | loss: 0.0251893\n",
      "\tspeed: 0.1456s/iter; left time: 203.4149s\n",
      "Epoch: 3 cost time: 37.43153405189514\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.0266075 Vali Loss: 0.0292618 Test Loss: 0.0297471\n",
      "Validation loss decreased (0.045773 --> 0.029262).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0202201\n",
      "\tspeed: 0.3408s/iter; left time: 419.4884s\n",
      "\titers: 200, epoch: 4 | loss: 0.0231499\n",
      "\tspeed: 0.1474s/iter; left time: 166.7503s\n",
      "Epoch: 4 cost time: 39.19699954986572\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.0225791 Vali Loss: 0.0323898 Test Loss: 0.0320516\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0194972\n",
      "\tspeed: 0.3388s/iter; left time: 326.8996s\n",
      "\titers: 200, epoch: 5 | loss: 0.0214950\n",
      "\tspeed: 0.1482s/iter; left time: 128.1817s\n",
      "Epoch: 5 cost time: 38.45511817932129\n",
      "Epoch: 5, Steps: 266 | Train Loss: 0.0213005 Vali Loss: 0.0307097 Test Loss: 0.0311951\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0262282\n",
      "\tspeed: 0.3024s/iter; left time: 211.3484s\n",
      "\titers: 200, epoch: 6 | loss: 0.0190445\n",
      "\tspeed: 0.1475s/iter; left time: 88.3441s\n",
      "Epoch: 6 cost time: 39.176613330841064\n",
      "Epoch: 6, Steps: 266 | Train Loss: 0.0204968 Vali Loss: 0.0253789 Test Loss: 0.0263420\n",
      "Validation loss decreased (0.029262 --> 0.025379).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0171007\n",
      "\tspeed: 0.3410s/iter; left time: 147.6529s\n",
      "\titers: 200, epoch: 7 | loss: 0.0197117\n",
      "\tspeed: 0.1485s/iter; left time: 49.4411s\n",
      "Epoch: 7 cost time: 39.39856576919556\n",
      "Epoch: 7, Steps: 266 | Train Loss: 0.0199023 Vali Loss: 0.0240748 Test Loss: 0.0249990\n",
      "Validation loss decreased (0.025379 --> 0.024075).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0177807\n",
      "\tspeed: 0.3431s/iter; left time: 57.2916s\n",
      "\titers: 200, epoch: 8 | loss: 0.0177348\n",
      "\tspeed: 0.1475s/iter; left time: 9.8823s\n",
      "Epoch: 8 cost time: 36.485780239105225\n",
      "Epoch: 8, Steps: 266 | Train Loss: 0.0197047 Vali Loss: 0.0248656 Test Loss: 0.0255376\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      ">>>>>>>testing : informerstack_SYNTHh1_ftS_sl96_ll48_pl24_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2857\n",
      "test shape: (89, 32, 24, 1) (89, 32, 24, 1)\n",
      "test shape: (2848, 24, 1) (2848, 24, 1)\n",
      "mse:0.024920832365751266, mae:0.1255515217781067\n"
     ]
    }
   ],
   "source": [
    "# train and predict 24 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_synth_24 = Exp_synth_24(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_synth_24.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    synth_pred_24, synth_true_24, synth_mse_24, synth_mae_24, synth_24_first_batch_test = exp_synth_24.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2) 168-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'SYNTHh1', 'root_path': './SYNTHDataset/', 'data_path': 'SYNTHh1.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 168, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Increase prediction length\n",
    "args.pred_len = 168 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_synth_168 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_SYNTHh1_ftS_sl96_ll48_pl168_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8377\n",
      "val 2713\n",
      "test 2713\n",
      "\titers: 100, epoch: 1 | loss: 0.2414376\n",
      "\tspeed: 0.2124s/iter; left time: 422.3871s\n",
      "\titers: 200, epoch: 1 | loss: 0.0868091\n",
      "\tspeed: 0.2148s/iter; left time: 405.6950s\n",
      "Epoch: 1 cost time: 55.95195031166077\n",
      "Epoch: 1, Steps: 261 | Train Loss: 0.2622942 Vali Loss: 0.1683587 Test Loss: 0.1533240\n",
      "Validation loss decreased (inf --> 0.168359).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0605847\n",
      "\tspeed: 0.4739s/iter; left time: 818.8904s\n",
      "\titers: 200, epoch: 2 | loss: 0.0458365\n",
      "\tspeed: 0.2149s/iter; left time: 349.8871s\n",
      "Epoch: 2 cost time: 53.554131507873535\n",
      "Epoch: 2, Steps: 261 | Train Loss: 0.0608510 Vali Loss: 0.1112561 Test Loss: 0.1085942\n",
      "Validation loss decreased (0.168359 --> 0.111256).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0473631\n",
      "\tspeed: 0.5027s/iter; left time: 737.4985s\n",
      "\titers: 200, epoch: 3 | loss: 0.0302710\n",
      "\tspeed: 0.2148s/iter; left time: 293.6874s\n",
      "Epoch: 3 cost time: 56.13202691078186\n",
      "Epoch: 3, Steps: 261 | Train Loss: 0.0397620 Vali Loss: 0.0830881 Test Loss: 0.0867405\n",
      "Validation loss decreased (0.111256 --> 0.083088).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0373799\n",
      "\tspeed: 0.4758s/iter; left time: 573.8384s\n",
      "\titers: 200, epoch: 4 | loss: 0.0286217\n",
      "\tspeed: 0.2125s/iter; left time: 235.0637s\n",
      "Epoch: 4 cost time: 53.433438777923584\n",
      "Epoch: 4, Steps: 261 | Train Loss: 0.0335564 Vali Loss: 0.1065375 Test Loss: 0.1097763\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0302864\n",
      "\tspeed: 0.4956s/iter; left time: 468.3162s\n",
      "\titers: 200, epoch: 5 | loss: 0.0295147\n",
      "\tspeed: 0.2148s/iter; left time: 181.4719s\n",
      "Epoch: 5 cost time: 55.87813186645508\n",
      "Epoch: 5, Steps: 261 | Train Loss: 0.0287875 Vali Loss: 0.0838564 Test Loss: 0.0855295\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0272671\n",
      "\tspeed: 0.4756s/iter; left time: 325.3157s\n",
      "\titers: 200, epoch: 6 | loss: 0.0258308\n",
      "\tspeed: 0.2146s/iter; left time: 125.3338s\n",
      "Epoch: 6 cost time: 53.900904178619385\n",
      "Epoch: 6, Steps: 261 | Train Loss: 0.0270610 Vali Loss: 0.1068945 Test Loss: 0.1159694\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : informerstack_SYNTHh1_ftS_sl96_ll48_pl168_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2713\n",
      "test shape: (84, 32, 168, 1) (84, 32, 168, 1)\n",
      "test shape: (2688, 168, 1) (2688, 168, 1)\n",
      "mse:0.08663419634103775, mae:0.23446011543273926\n"
     ]
    }
   ],
   "source": [
    "# Train and predict 168 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_synth_168 = Exp_synth_168(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_synth_168.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    synth_pred_168, synth_true_168, synth_mse_168, synth_mae_168, synth_168_first_batch_test = exp_synth_168.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3) 720-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'SYNTHh1', 'root_path': './SYNTHDataset/', 'data_path': 'SYNTHh1.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 720, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Increase prediction length\n",
    "args.pred_len = 720 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_synth_720 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_SYNTHh1_ftS_sl96_ll48_pl720_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7825\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.3717384\n",
      "\tspeed: 0.3830s/iter; left time: 709.7644s\n",
      "\titers: 200, epoch: 1 | loss: 0.3124363\n",
      "\tspeed: 0.4056s/iter; left time: 710.9963s\n",
      "Epoch: 1 cost time: 96.83217573165894\n",
      "Epoch: 1, Steps: 244 | Train Loss: 0.4210257 Vali Loss: 0.4713870 Test Loss: 0.4362603\n",
      "Validation loss decreased (inf --> 0.471387).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.3111594\n",
      "\tspeed: 0.8266s/iter; left time: 1330.0097s\n",
      "\titers: 200, epoch: 2 | loss: 0.0722514\n",
      "\tspeed: 0.3895s/iter; left time: 587.7827s\n",
      "Epoch: 2 cost time: 97.56407308578491\n",
      "Epoch: 2, Steps: 244 | Train Loss: 0.2055912 Vali Loss: 0.1760287 Test Loss: 0.1403989\n",
      "Validation loss decreased (0.471387 --> 0.176029).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0361420\n",
      "\tspeed: 0.8260s/iter; left time: 1127.5296s\n",
      "\titers: 200, epoch: 3 | loss: 0.0304314\n",
      "\tspeed: 0.4079s/iter; left time: 515.9894s\n",
      "Epoch: 3 cost time: 97.31784105300903\n",
      "Epoch: 3, Steps: 244 | Train Loss: 0.0387723 Vali Loss: 0.1896293 Test Loss: 0.1554782\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0313446\n",
      "\tspeed: 0.8061s/iter; left time: 903.6304s\n",
      "\titers: 200, epoch: 4 | loss: 0.0303228\n",
      "\tspeed: 0.4039s/iter; left time: 412.3581s\n",
      "Epoch: 4 cost time: 98.70987677574158\n",
      "Epoch: 4, Steps: 244 | Train Loss: 0.0312703 Vali Loss: 0.1522557 Test Loss: 0.1198907\n",
      "Validation loss decreased (0.176029 --> 0.152256).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0274295\n",
      "\tspeed: 0.8090s/iter; left time: 709.5344s\n",
      "\titers: 200, epoch: 5 | loss: 0.0269346\n",
      "\tspeed: 0.4022s/iter; left time: 312.4898s\n",
      "Epoch: 5 cost time: 96.87932252883911\n",
      "Epoch: 5, Steps: 244 | Train Loss: 0.0281364 Vali Loss: 0.1570461 Test Loss: 0.1224263\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0277483\n",
      "\tspeed: 0.8202s/iter; left time: 519.2172s\n",
      "\titers: 200, epoch: 6 | loss: 0.0269862\n",
      "\tspeed: 0.3773s/iter; left time: 201.1236s\n",
      "Epoch: 6 cost time: 95.62249183654785\n",
      "Epoch: 6, Steps: 244 | Train Loss: 0.0268734 Vali Loss: 0.1463084 Test Loss: 0.1173612\n",
      "Validation loss decreased (0.152256 --> 0.146308).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0249905\n",
      "\tspeed: 0.8112s/iter; left time: 315.5622s\n",
      "\titers: 200, epoch: 7 | loss: 0.0260920\n",
      "\tspeed: 0.3993s/iter; left time: 115.4015s\n",
      "Epoch: 7 cost time: 97.40098524093628\n",
      "Epoch: 7, Steps: 244 | Train Loss: 0.0261928 Vali Loss: 0.1456170 Test Loss: 0.1154429\n",
      "Validation loss decreased (0.146308 --> 0.145617).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0248933\n",
      "\tspeed: 0.8003s/iter; left time: 116.0475s\n",
      "\titers: 200, epoch: 8 | loss: 0.0251537\n",
      "\tspeed: 0.4015s/iter; left time: 18.0658s\n",
      "Epoch: 8 cost time: 98.15618848800659\n",
      "Epoch: 8, Steps: 244 | Train Loss: 0.0256616 Vali Loss: 0.1374450 Test Loss: 0.1087613\n",
      "Validation loss decreased (0.145617 --> 0.137445).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      ">>>>>>>testing : informerstack_SYNTHh1_ftS_sl96_ll48_pl720_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2161\n",
      "test shape: (67, 32, 720, 1) (67, 32, 720, 1)\n",
      "test shape: (2144, 720, 1) (2144, 720, 1)\n",
      "mse:0.10871177911758423, mae:0.2598571181297302\n"
     ]
    }
   ],
   "source": [
    "# Train and predict 720 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_synth_720 = Exp_synth_720(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_synth_720.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    synth_pred_720, synth_true_720, synth_mse_720, synth_mae_720, synth_720_first_batch_test = exp_synth_720.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Wind Data Forecasting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1) 24-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'DEWINDh_small', 'root_path': './WINDataset/', 'data_path': 'DEWINDh_small.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Set data to wind\n",
    "args.data = 'DEWINDh_small' # data\n",
    "args.root_path = './WINDataset/' # root path of data file\n",
    "args.data_path = 'DEWINDh_small.csv' # data file\n",
    "\n",
    "# Re-set prediction length to 24\n",
    "args.pred_len = 24 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_wind_24 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_DEWINDh_small_ftS_sl96_ll48_pl24_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.1922024\n",
      "\tspeed: 0.1273s/iter; left time: 258.3546s\n",
      "\titers: 200, epoch: 1 | loss: 0.1572273\n",
      "\tspeed: 0.1439s/iter; left time: 277.4938s\n",
      "Epoch: 1 cost time: 36.89381670951843\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.2932847 Vali Loss: 0.1607707 Test Loss: 0.1650750\n",
      "Validation loss decreased (inf --> 0.160771).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1601011\n",
      "\tspeed: 0.3434s/iter; left time: 605.3271s\n",
      "\titers: 200, epoch: 2 | loss: 0.1065265\n",
      "\tspeed: 0.1480s/iter; left time: 246.1596s\n",
      "Epoch: 2 cost time: 39.424134969711304\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.1376669 Vali Loss: 0.1673882 Test Loss: 0.1686659\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1297745\n",
      "\tspeed: 0.3446s/iter; left time: 515.9154s\n",
      "\titers: 200, epoch: 3 | loss: 0.0798464\n",
      "\tspeed: 0.1491s/iter; left time: 208.2354s\n",
      "Epoch: 3 cost time: 39.49358344078064\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.1123658 Vali Loss: 0.1545454 Test Loss: 0.1503391\n",
      "Validation loss decreased (0.160771 --> 0.154545).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1389164\n",
      "\tspeed: 0.3134s/iter; left time: 385.8012s\n",
      "\titers: 200, epoch: 4 | loss: 0.1263046\n",
      "\tspeed: 0.1476s/iter; left time: 166.8957s\n",
      "Epoch: 4 cost time: 37.23397207260132\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.1012639 Vali Loss: 0.1497261 Test Loss: 0.1388970\n",
      "Validation loss decreased (0.154545 --> 0.149726).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1330473\n",
      "\tspeed: 0.3440s/iter; left time: 331.9991s\n",
      "\titers: 200, epoch: 5 | loss: 0.1168147\n",
      "\tspeed: 0.1485s/iter; left time: 128.4120s\n",
      "Epoch: 5 cost time: 39.43747663497925\n",
      "Epoch: 5, Steps: 266 | Train Loss: 0.0945155 Vali Loss: 0.1545800 Test Loss: 0.1406909\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0584799\n",
      "\tspeed: 0.3430s/iter; left time: 239.7633s\n",
      "\titers: 200, epoch: 6 | loss: 0.0933561\n",
      "\tspeed: 0.1478s/iter; left time: 88.5183s\n",
      "Epoch: 6 cost time: 38.833149433135986\n",
      "Epoch: 6, Steps: 266 | Train Loss: 0.0907391 Vali Loss: 0.1458654 Test Loss: 0.1345409\n",
      "Validation loss decreased (0.149726 --> 0.145865).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1455973\n",
      "\tspeed: 0.3081s/iter; left time: 133.4170s\n",
      "\titers: 200, epoch: 7 | loss: 0.0853310\n",
      "\tspeed: 0.1496s/iter; left time: 49.8041s\n",
      "Epoch: 7 cost time: 39.54373025894165\n",
      "Epoch: 7, Steps: 266 | Train Loss: 0.0881308 Vali Loss: 0.1507612 Test Loss: 0.1367768\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.0692126\n",
      "\tspeed: 0.3428s/iter; left time: 57.2432s\n",
      "\titers: 200, epoch: 8 | loss: 0.1031156\n",
      "\tspeed: 0.1475s/iter; left time: 9.8843s\n",
      "Epoch: 8 cost time: 39.49782037734985\n",
      "Epoch: 8, Steps: 266 | Train Loss: 0.0865138 Vali Loss: 0.1515143 Test Loss: 0.1354875\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 7.8125e-07\n",
      ">>>>>>>testing : informerstack_DEWINDh_small_ftS_sl96_ll48_pl24_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2857\n",
      "test shape: (89, 32, 24, 1) (89, 32, 24, 1)\n",
      "test shape: (2848, 24, 1) (2848, 24, 1)\n",
      "mse:0.13438719511032104, mae:0.20457372069358826\n"
     ]
    }
   ],
   "source": [
    "# train and predict 24 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_wind_24 = Exp_wind_24(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_wind_24.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    wind_pred_24, wind_true_24, wind_mse_24, wind_mae_24, wind_24_first_batch_test = exp_wind_24.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2) 168-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'DEWINDh_small', 'root_path': './WINDataset/', 'data_path': 'DEWINDh_small.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 168, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Increase prediction length\n",
    "args.pred_len = 168 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_wind_168 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_DEWINDh_small_ftS_sl96_ll48_pl168_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8377\n",
      "val 2713\n",
      "test 2713\n",
      "\titers: 100, epoch: 1 | loss: 0.2307981\n",
      "\tspeed: 0.2120s/iter; left time: 421.7603s\n",
      "\titers: 200, epoch: 1 | loss: 0.1850148\n",
      "\tspeed: 0.1871s/iter; left time: 353.4714s\n",
      "Epoch: 1 cost time: 53.085575342178345\n",
      "Epoch: 1, Steps: 261 | Train Loss: 0.3990811 Vali Loss: 0.1926250 Test Loss: 0.1978261\n",
      "Validation loss decreased (inf --> 0.192625).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1603144\n",
      "\tspeed: 0.4977s/iter; left time: 860.1030s\n",
      "\titers: 200, epoch: 2 | loss: 0.1607429\n",
      "\tspeed: 0.2151s/iter; left time: 350.1922s\n",
      "Epoch: 2 cost time: 56.17479872703552\n",
      "Epoch: 2, Steps: 261 | Train Loss: 0.1606259 Vali Loss: 0.2184845 Test Loss: 0.2397944\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1295068\n",
      "\tspeed: 0.4975s/iter; left time: 729.8002s\n",
      "\titers: 200, epoch: 3 | loss: 0.1550812\n",
      "\tspeed: 0.1874s/iter; left time: 256.1888s\n",
      "Epoch: 3 cost time: 53.120322465896606\n",
      "Epoch: 3, Steps: 261 | Train Loss: 0.1403636 Vali Loss: 0.1950368 Test Loss: 0.2008934\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1067434\n",
      "\tspeed: 0.4985s/iter; left time: 601.2073s\n",
      "\titers: 200, epoch: 4 | loss: 0.1258378\n",
      "\tspeed: 0.2147s/iter; left time: 237.5089s\n",
      "Epoch: 4 cost time: 56.14466166496277\n",
      "Epoch: 4, Steps: 261 | Train Loss: 0.1291572 Vali Loss: 0.1929403 Test Loss: 0.1989550\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : informerstack_DEWINDh_small_ftS_sl96_ll48_pl168_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2713\n",
      "test shape: (84, 32, 168, 1) (84, 32, 168, 1)\n",
      "test shape: (2688, 168, 1) (2688, 168, 1)\n",
      "mse:0.19749152660369873, mae:0.28165727853775024\n"
     ]
    }
   ],
   "source": [
    "# Train and predict 168 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_wind_168 = Exp_wind_168(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_wind_168.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    wind_pred_168, wind_true_168, wind_mse_168, wind_mae_168, wind_168_first_batch_test = exp_wind_168.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3) 720-Hour Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informerstack', 'data': 'DEWINDh_small', 'root_path': './WINDataset/', 'data_path': 'DEWINDh_small.csv', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 720, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 8, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "# Increase prediction length\n",
    "args.pred_len = 720 \n",
    "\n",
    "# Inspect new experiment arguments\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "# Initialize informer \n",
    "Exp_wind_720 = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_DEWINDh_small_ftS_sl96_ll48_pl720_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7825\n",
      "val 2161\n",
      "test 2161\n",
      "\titers: 100, epoch: 1 | loss: 0.9777046\n",
      "\tspeed: 0.3842s/iter; left time: 711.8428s\n",
      "\titers: 200, epoch: 1 | loss: 0.4322140\n",
      "\tspeed: 0.4028s/iter; left time: 706.1086s\n",
      "Epoch: 1 cost time: 96.67148399353027\n",
      "Epoch: 1, Steps: 244 | Train Loss: 0.8364656 Vali Loss: 0.2677885 Test Loss: 0.2772596\n",
      "Validation loss decreased (inf --> 0.267788).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1833613\n",
      "\tspeed: 0.8226s/iter; left time: 1323.6332s\n",
      "\titers: 200, epoch: 2 | loss: 0.1839741\n",
      "\tspeed: 0.3829s/iter; left time: 577.7579s\n",
      "Epoch: 2 cost time: 96.47203469276428\n",
      "Epoch: 2, Steps: 244 | Train Loss: 0.1913905 Vali Loss: 0.2042943 Test Loss: 0.2050335\n",
      "Validation loss decreased (0.267788 --> 0.204294).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1446158\n",
      "\tspeed: 0.8218s/iter; left time: 1121.7964s\n",
      "\titers: 200, epoch: 3 | loss: 0.1715730\n",
      "\tspeed: 0.4046s/iter; left time: 511.8437s\n",
      "Epoch: 3 cost time: 98.74606537818909\n",
      "Epoch: 3, Steps: 244 | Train Loss: 0.1656810 Vali Loss: 0.1943359 Test Loss: 0.1979598\n",
      "Validation loss decreased (0.204294 --> 0.194336).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1554669\n",
      "\tspeed: 0.8036s/iter; left time: 900.7842s\n",
      "\titers: 200, epoch: 4 | loss: 0.1513847\n",
      "\tspeed: 0.4056s/iter; left time: 414.1179s\n",
      "Epoch: 4 cost time: 98.91705536842346\n",
      "Epoch: 4, Steps: 244 | Train Loss: 0.1569447 Vali Loss: 0.2056397 Test Loss: 0.1974251\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1514461\n",
      "\tspeed: 0.8125s/iter; left time: 712.5823s\n",
      "\titers: 200, epoch: 5 | loss: 0.1605678\n",
      "\tspeed: 0.3940s/iter; left time: 306.1099s\n",
      "Epoch: 5 cost time: 96.63426756858826\n",
      "Epoch: 5, Steps: 244 | Train Loss: 0.1517991 Vali Loss: 0.2080687 Test Loss: 0.1979491\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1526878\n",
      "\tspeed: 0.8183s/iter; left time: 517.9890s\n",
      "\titers: 200, epoch: 6 | loss: 0.1462851\n",
      "\tspeed: 0.3995s/iter; left time: 212.9454s\n",
      "Epoch: 6 cost time: 96.44810175895691\n",
      "Epoch: 6, Steps: 244 | Train Loss: 0.1491097 Vali Loss: 0.2061851 Test Loss: 0.1968592\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : informerstack_DEWINDh_small_ftS_sl96_ll48_pl720_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 2161\n",
      "test shape: (67, 32, 720, 1) (67, 32, 720, 1)\n",
      "test shape: (2144, 720, 1) (2144, 720, 1)\n",
      "mse:0.19812017679214478, mae:0.2626587450504303\n"
     ]
    }
   ],
   "source": [
    "# Train and predict 720 hour windows\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.s_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "    # set experiments\n",
    "    exp_wind_720 = Exp_wind_720(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    \n",
    "    model = exp_wind_720.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    \n",
    "    # We can now return prediction windows, true value windows and metrics, as well as the first batch from the test set for possible trouble-shooting\n",
    "    wind_pred_720, wind_true_720, wind_mse_720, wind_mae_720, wind_720_first_batch_test = exp_wind_720.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anbs_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
