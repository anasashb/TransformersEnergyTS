{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from math import sqrt, ceil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-03 19:42:56.294689: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-03 19:42:57.840639: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-03 19:43:03.489085: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/RDC/anasashb/.conda/envs/anbs_2/lib/python3.8/site-packages/nvidia/cudnn/lib:/home/RDC/anasashb/.conda/envs/anbs_2/lib/:/home/RDC/anasashb/.conda/envs/anbs_2/lib/:/home/RDC/anasashb/.conda/envs/anbs_2/lib/python3.8/site-packages/nvidia/cudnn/lib:\n",
      "2024-01-03 19:43:03.489569: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/RDC/anasashb/.conda/envs/anbs_2/lib/python3.8/site-packages/nvidia/cudnn/lib:/home/RDC/anasashb/.conda/envs/anbs_2/lib/:/home/RDC/anasashb/.conda/envs/anbs_2/lib/:/home/RDC/anasashb/.conda/envs/anbs_2/lib/python3.8/site-packages/nvidia/cudnn/lib:\n",
      "2024-01-03 19:43:03.489591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "### TSMixer API\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "# Metrics #################################################################################################################\n",
    "### ALL METRICS ARE INTERCHANGEABLE WITH INFORMER AUTOFORMER\n",
    "def RSE(pred, true):\n",
    "    '''\n",
    "    Calculates relative quared error.\n",
    "    '''\n",
    "    return np.sqrt(np.sum((true-pred)**2)) / np.sqrt(np.sum((true-true.mean())**2))\n",
    "\n",
    "def CORR(pred, true):\n",
    "    '''\n",
    "    Calculates correlation coefficient.\n",
    "    '''\n",
    "    u = ((true-true.mean(0))*(pred-pred.mean(0))).sum(0) \n",
    "    d = np.sqrt(((true-true.mean(0))**2*(pred-pred.mean(0))**2).sum(0))\n",
    "    return (u/d).mean(-1)\n",
    "\n",
    "def MAE(pred, true):\n",
    "    '''\n",
    "    Calculates mean absolute error.\n",
    "    '''\n",
    "    return np.mean(np.abs(pred-true))\n",
    "\n",
    "def MSE(pred, true):\n",
    "    '''\n",
    "    Calculates mean squared error.\n",
    "    '''\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "def RMSE(pred, true):\n",
    "    '''\n",
    "    Calculates root mean suared error.\n",
    "    '''\n",
    "    return np.sqrt(MSE(pred, true))\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    '''\n",
    "    Calculates mean absolute percentage error.\n",
    "    '''\n",
    "    return np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "def MSPE(pred, true):\n",
    "    '''\n",
    "    Calculates mean squared percentage error.\n",
    "    '''\n",
    "    return np.mean(np.square((pred - true) / true))\n",
    "\n",
    "def metric(pred, true):\n",
    "    '''\n",
    "    Wraps up metric functions, calculates and returns all.\n",
    "    '''\n",
    "    mae = MAE(pred, true)\n",
    "    mse = MSE(pred, true)\n",
    "    rmse = RMSE(pred, true)\n",
    "    mape = MAPE(pred, true)\n",
    "    mspe = MSPE(pred, true)\n",
    "    \n",
    "    return mae,mse,rmse,mape,mspe\n",
    "\n",
    "# Dot dictionary ##########################################################################################################\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "########## TODOOOO\n",
    "\n",
    "###########################################################################################################################\n",
    "#  Data loader and dependencies ###########################################################################################  \n",
    "\n",
    "## TODO ### \n",
    "## Delete the DATA_DIR CODE\n",
    "## Change LOCAL_CACHE_DIR to data_root_path variable that can be added to the fit method.\n",
    "## Will need to be set as self argument to loader class also\n",
    "        \n",
    "\n",
    "DATA_DIR = 'gs://time_series_datasets'\n",
    "LOCAL_CACHE_DIR = './dataset/'\n",
    "\n",
    "class TSFDataLoader:\n",
    "    \"\"\"Generate data loader from raw data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, data, batch_size, seq_len, pred_len, feature_type, target='OT'\n",
    "            ):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.feature_type = feature_type\n",
    "        self.target = target\n",
    "        self.target_slice = slice(0, None)\n",
    "\n",
    "        self._read_data()\n",
    "\n",
    "    def _read_data(self):\n",
    "        \"\"\"Load raw data and split datasets.\"\"\"\n",
    "\n",
    "        # copy data from cloud storage if not exists\n",
    "        if not os.path.isdir(LOCAL_CACHE_DIR):\n",
    "            os.mkdir(LOCAL_CACHE_DIR)\n",
    "\n",
    "        file_name = self.data + '.csv'\n",
    "        cache_filepath = os.path.join(LOCAL_CACHE_DIR, file_name)\n",
    "        if not os.path.isfile(cache_filepath):\n",
    "            tf.io.gfile.copy(\n",
    "                os.path.join(DATA_DIR, file_name), cache_filepath, overwrite=True\n",
    "                )\n",
    "\n",
    "        df_raw = pd.read_csv(cache_filepath)\n",
    "\n",
    "        # S: univariate-univariate, M: multivariate-multivariate, MS:\n",
    "        # multivariate-univariate\n",
    "        df = df_raw.set_index('date')\n",
    "        if self.feature_type == 'S':\n",
    "            df = df[[self.target]]\n",
    "        elif self.feature_type == 'MS':\n",
    "            target_idx = df.columns.get_loc(self.target)\n",
    "            self.target_slice = slice(target_idx, target_idx + 1)\n",
    "\n",
    "        # split train/valid/test\n",
    "        n = len(df)\n",
    "        if self.data.startswith('ETTm'):\n",
    "            train_end = 12 * 30 * 24 * 4\n",
    "            val_end = train_end + 4 * 30 * 24 * 4\n",
    "            test_end = val_end + 4 * 30 * 24 * 4\n",
    "    \n",
    "        # THE SPLITS below match the splits of Informer, Crossformer, Autoformer, Fedformer\n",
    "        elif self.data.startswith('ETTh'):\n",
    "            train_end = 12 * 30 * 24\n",
    "            val_end = train_end + 4 * 30 * 24\n",
    "            test_end = val_end + 4 * 30 * 24\n",
    "        # I added two more elifs for synth and wind data, we can do the split provision here too\n",
    "        elif self.data.startswith('SYNTHh'):\n",
    "            train_end = 12 * 30 * 24\n",
    "            val_end = train_end + 4 * 30 * 24\n",
    "            test_end = val_end + 4 * 30 * 24\n",
    "        elif self.data.startswith('DEWINDh'):\n",
    "            train_end = 12 * 30 * 24\n",
    "            val_end = train_end + 4 * 30 * 24\n",
    "            test_end = val_end + 4 * 30 * 24\n",
    "        else:\n",
    "            train_end = int(n * 0.7)\n",
    "            val_end = n - int(n * 0.2)\n",
    "            test_end = n\n",
    "\n",
    "        train_df = df[:train_end]\n",
    "        val_df = df[train_end - self.seq_len : val_end]\n",
    "        test_df = df[val_end - self.seq_len : test_end]\n",
    "\n",
    "        # Debug debug\n",
    "        print(len(train_df))\n",
    "        print(len(val_df))\n",
    "        print(len(test_df))\n",
    "        # Debug debug\n",
    "\n",
    "        # standardize by training set\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(train_df.values)\n",
    "\n",
    "        def scale_df(df, scaler):\n",
    "            data = scaler.transform(df.values)\n",
    "            return pd.DataFrame(data, index=df.index, columns=df.columns)\n",
    "\n",
    "        self.train_df = scale_df(train_df, self.scaler)\n",
    "        self.val_df = scale_df(val_df, self.scaler)\n",
    "        self.test_df = scale_df(test_df, self.scaler)\n",
    "        self.n_feature = self.train_df.shape[-1]\n",
    "\n",
    "    def _split_window(self, data):\n",
    "        inputs = data[:, : self.seq_len, :]\n",
    "        labels = data[:, self.seq_len :, self.target_slice]\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.seq_len, None])\n",
    "        labels.set_shape([None, self.pred_len, None])\n",
    "        return inputs, labels\n",
    "\n",
    "    def _make_dataset(self, data, shuffle=True):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=(self.seq_len + self.pred_len),\n",
    "            sequence_stride=1, # window stride\n",
    "            shuffle=shuffle,\n",
    "            batch_size=self.batch_size,\n",
    "            )\n",
    "        ds = ds.map(self._split_window)\n",
    "        return ds\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "    def get_train(self, shuffle=True):\n",
    "        return self._make_dataset(self.train_df, shuffle=shuffle)\n",
    "\n",
    "    def get_val(self):\n",
    "        return self._make_dataset(self.val_df, shuffle=False)\n",
    "\n",
    "    def get_test(self):\n",
    "        return self._make_dataset(self.test_df, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "# Reversible Instance Normalization #######################################################################################\n",
    "class RevNorm(layers.Layer):\n",
    "    \"\"\"Reversible Instance Normalization.\"\"\"\n",
    "\n",
    "    def __init__(self, axis, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.affine:\n",
    "            self.affine_weight = self.add_weight(\n",
    "               'affine_weight', shape=input_shape[-1], initializer='ones'\n",
    "               )\n",
    "            self.affine_bias = self.add_weight(\n",
    "               'affine_bias', shape=input_shape[-1], initializer='zeros'\n",
    "               )\n",
    "\n",
    "    def call(self, x, mode, target_slice=None):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x, target_slice)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        self.mean = tf.stop_gradient(\n",
    "           tf.reduce_mean(x, axis=self.axis, keepdims=True)\n",
    "           )\n",
    "        self.stdev = tf.stop_gradient(\n",
    "           tf.sqrt(\n",
    "              tf.math.reduce_variance(x, axis=self.axis, keepdims=True) + self.eps\n",
    "              )\n",
    "            )\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x, target_slice=None):\n",
    "        if self.affine:\n",
    "           x = x - self.affine_bias[target_slice]\n",
    "           x = x / self.affine_weight[target_slice]\n",
    "        x = x * self.stdev[:, :, target_slice]\n",
    "        x = x + self.mean[:, :, target_slice]\n",
    "        return x\n",
    "  \n",
    "\n",
    "###########################################################################################################################\n",
    "# TSMIxer Block ###########################################################################################################\n",
    "def res_block(inputs, norm_type, activation, dropout, ff_dim):\n",
    "    \"\"\"Residual block of TSMixer.\"\"\"\n",
    "\n",
    "    norm = (\n",
    "       layers.LayerNormalization if norm_type == 'L' else layers.BatchNormalization\n",
    "       )\n",
    "\n",
    "    # Temporal Linear\n",
    "    x = norm(axis=[-2, -1])(inputs)\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
    "    x = layers.Dense(x.shape[-1], activation=activation)(x)\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Input Length, Channel]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "  # Feature Linear\n",
    "    x = norm(axis=[-2, -1])(res)\n",
    "    x = layers.Dense(ff_dim, activation=activation)(\n",
    "       x\n",
    "    )  # [Batch, Input Length, FF_Dim]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    return x + res    \n",
    "  \n",
    "###########################################################################################################################\n",
    "# Build TSMixer with Reversible Instance Normalization ####################################################################\n",
    "def build_model(\n",
    "      input_shape,\n",
    "      pred_len,\n",
    "      norm_type,\n",
    "      activation,\n",
    "      n_block,\n",
    "      dropout,\n",
    "      ff_dim,\n",
    "      target_slice,\n",
    "    ):\n",
    "    \n",
    "    \"\"\"Build TSMixer with Reversible Instance Normalization model.\"\"\"\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs  # [Batch, Input Length, Channel]\n",
    "    rev_norm = RevNorm(axis=-2)\n",
    "    x = rev_norm(x, 'norm')\n",
    "    for _ in range(n_block):\n",
    "        x = res_block(x, norm_type, activation, dropout, ff_dim)\n",
    "\n",
    "    if target_slice:\n",
    "        x = x[:, :, target_slice]\n",
    "\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
    "    x = layers.Dense(pred_len)(x)  # [Batch, Channel, Output Length]\n",
    "    outputs = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Output Length, Channel])\n",
    "    outputs = rev_norm(outputs, 'denorm', target_slice)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "class TSMixer():\n",
    "    '''\n",
    "    I am so thoroughly exhausted you cannot imagine\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model='tsmixer_rev_in'):\n",
    "        self.args = dotdict()\n",
    "        self.args.model = model ## I keep these redundant model args to maybe then combine all API-s\n",
    "        self.args.seed = 0\n",
    "        \n",
    "        ## Variables for Multivariate ##################################################################\n",
    "        ## TODO ## \n",
    "        ## I need to take this outside in a method called get_data maybe?\n",
    "        ## Choices for this and other API-s must be ['S', 'M', 'MS] <-- check if Crossformer can handle\n",
    "        self.args.features = 'S' ## currently tailored to synth and wind series\n",
    "        self.args.target = 'TARGET' ## Because I give this name to the column\n",
    "        ################################################################################################\n",
    "        \n",
    "        \n",
    "        self.checkpoints = './tsmixer_checkpoints'\n",
    "        self.delete_checkpoint = False ## I am not sure this is correct default\n",
    "\n",
    "        ## Variables for TS\n",
    "        self.args.seq_len = 96 # used the default of other models, authors set it to 336\n",
    "\n",
    "        # Model Architecture\n",
    "        #self.kernel_size = 4 ## deactivated because we do not fit CNN\n",
    "        self.args.n_block = 2 ## number of blocks for deep architecture\n",
    "        self.args.ff_dim = 2048 ## fully-connected feature dimension\n",
    "        self.args.dropout = 0.05 ## dropout rate\n",
    "        self.args.norm_type = 'B' ## BatchNorm. Authors included alternative -- 'L' LayerNorm\n",
    "        self.args.activation = 'relu' ## Authors included possible alternative -- 'gelu'\n",
    "        self.args.temporal_dim = 16 ## temporal feature dimension\n",
    "        self.args.hidden_dim = 64 ## hidden feature dimension\n",
    "        self.args.num_workers = 19 ## maybe switch this to 0 like other models if there is a problem\n",
    "\n",
    "\n",
    "    def compile(self, learning_rate=1e-4, loss='mse', early_stopping_patience=5):\n",
    "        ## should include\n",
    "        ## loss, \n",
    "        if loss != 'mse':\n",
    "            raise ValueError(\"Loss function not supported. Please use 'mse'.\")\n",
    "        self.args.loss = loss\n",
    "        self.args.learning_rate = learning_rate\n",
    "        self.args.patience = early_stopping_patience\n",
    "\n",
    "    \n",
    "    def fit(self, batch_size=32, epochs=100, pred_len=24):\n",
    "        ## Should include\n",
    "        ## data, data_root_path, batch_size, epochs, pred_len\n",
    "        possible_predlens = [24, 48, 96, 168, 336, 720]\n",
    "        if pred_len not in possible_predlens:\n",
    "            raise ValueError('Prediction length outside current experiment scope. Please use either 24, 48, 96, 168, 336, 720.')\n",
    "        self.args.pred_len = pred_len\n",
    "        self.args.batch_size = batch_size ## 32 is the authors' default\n",
    "        self.args.train_epochs = epochs ## 100 is the authors' default\n",
    "    # optimization\n",
    "\n",
    "        return 'lol good luck'\n",
    "    \n",
    "    def predict(self):\n",
    "        return 'lol good luck'\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse the arguments for experiment configuration.\"\"\"\n",
    "\n",
    "    # data loader\n",
    "    parser.add_argument(\n",
    "        '--data',\n",
    "        type=str,\n",
    "        default='weather',\n",
    "        choices=[\n",
    "            'electricity',\n",
    "            'exchange_rate',\n",
    "            'national_illness',\n",
    "            'traffic',\n",
    "            'weather',\n",
    "            'ETTm1',\n",
    "            'ETTm2',\n",
    "            'ETTh1',\n",
    "            'ETTh2',\n",
    "        ],\n",
    "        help='data name',\n",
    "    )\n",
    "\n",
    "\n",
    "    # save results\n",
    "    parser.add_argument(\n",
    "        '--result_path', default='result.csv', help='path to save result'\n",
    "    )\n",
    "\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    tf.keras.utils.set_random_seed(self.args.seed)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    if 'tsmixer' in args.model:\n",
    "        exp_id = f'{args.data}_{args.feature_type}_{args.model}_sl{args.seq_len}_pl{args.pred_len}_lr{args.learning_rate}_nt{args.norm_type}_{args.activation}_nb{args.n_block}_dp{args.dropout}_fd{args.ff_dim}'\n",
    "    elif args.model == 'full_linear':\n",
    "        exp_id = f'{args.data}_{args.feature_type}_{args.model}_sl{args.seq_len}_pl{args.pred_len}_lr{args.learning_rate}'\n",
    "    elif args.model == 'cnn':\n",
    "        exp_id = f'{args.data}_{args.feature_type}_{args.model}_sl{args.seq_len}_pl{args.pred_len}_lr{args.learning_rate}_ks{args.kernel_size}'\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model type: {args.model}')\n",
    "\n",
    "    # load datasets\n",
    "    data_loader = TSFDataLoader(\n",
    "        args.data,\n",
    "        args.batch_size,\n",
    "        args.seq_len,\n",
    "        args.pred_len,\n",
    "        args.feature_type,\n",
    "        args.target,\n",
    "    )\n",
    "    train_data = data_loader.get_train()\n",
    "    val_data = data_loader.get_val()\n",
    "    test_data = data_loader.get_test()\n",
    "\n",
    "    ### TODO\n",
    "    ## Where the hell is this models variable coming from\n",
    "\n",
    "    # train model\n",
    "    if 'tsmixer' in args.model:\n",
    "        build_model = getattr(models, args.model).build_model\n",
    "        model = build_model(\n",
    "            input_shape=(args.seq_len, data_loader.n_feature),\n",
    "            pred_len=args.pred_len,\n",
    "            norm_type=args.norm_type,\n",
    "            activation=args.activation,\n",
    "            dropout=args.dropout,\n",
    "            n_block=args.n_block,\n",
    "            ff_dim=args.ff_dim,\n",
    "            target_slice=data_loader.target_slice,\n",
    "        )\n",
    "    elif args.model == 'full_linear':\n",
    "        model = models.full_linear.Model(\n",
    "            n_channel=data_loader.n_feature,\n",
    "            pred_len=args.pred_len,\n",
    "        )\n",
    "    elif args.model == 'cnn':\n",
    "        model = models.cnn.Model(\n",
    "            n_channel=data_loader.n_feature,\n",
    "            pred_len=args.pred_len,\n",
    "            kernel_size=args.kernel_size,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'Model not supported: {args.model}')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=self.args.loss, metrics=['mae'])\n",
    "    checkpoint_path = os.path.join(args.checkpoint_dir, f'{exp_id}_best')\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "    early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=args.patience\n",
    "    )\n",
    "    start_training_time = time.time()\n",
    "    history = model.fit(\n",
    "       train_data,\n",
    "       epochs=args.train_epochs,\n",
    "       validation_data=val_data,\n",
    "       callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    )\n",
    "    end_training_time = time.time()\n",
    "    elasped_training_time = end_training_time - start_training_time\n",
    "    print(f'Training finished in {elasped_training_time} secconds')\n",
    "\n",
    "    # evaluate best model\n",
    "    best_epoch = np.argmin(history.history['val_loss'])\n",
    "    model.load_weights(checkpoint_path)\n",
    "    test_result = model.evaluate(test_data)\n",
    "    if args.delete_checkpoint:\n",
    "        for f in glob.glob(checkpoint_path + '*'):\n",
    "            os.remove(f)\n",
    "\n",
    "    # save result to csv\n",
    "    data = {\n",
    "        'data': [args.data],\n",
    "        'model': [args.model],\n",
    "        'seq_len': [args.seq_len],\n",
    "        'pred_len': [args.pred_len],\n",
    "        'lr': [args.learning_rate],\n",
    "        'mse': [test_result[0]],\n",
    "        'mae': [test_result[1]],\n",
    "        'val_mse': [history.history['val_loss'][best_epoch]],\n",
    "        'val_mae': [history.history['val_mae'][best_epoch]],\n",
    "        'train_mse': [history.history['loss'][best_epoch]],\n",
    "        'train_mae': [history.history['mae'][best_epoch]],\n",
    "        'training_time': elasped_training_time,\n",
    "        'norm_type': args.norm_type,\n",
    "        'activation': args.activation,\n",
    "        'n_block': args.n_block,\n",
    "        'dropout': args.dropout,\n",
    "    }\n",
    "    \n",
    "    if 'TSMixer' in args.model:\n",
    "        data['ff_dim'] = args.ff_dim\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    if os.path.exists(args.result_path):\n",
    "        df.to_csv(args.result_path, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        df.to_csv(args.result_path, mode='w', index=False, header=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anbs_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
