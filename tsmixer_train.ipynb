{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from math import sqrt, ceil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 18:05:06.208023: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-04 18:05:06.299202: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-04 18:05:06.299242: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-04 18:05:06.299264: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-04 18:05:06.311208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "### TSMixer API\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "# NOTE maybe I should take this outside\n",
    "def drop_last_for_tensorflow(df, batch_size, seq_len, pred_len):\n",
    "    '''\n",
    "    Emulates PyTorch dataloaders' option for drop_last = True\n",
    "    '''\n",
    "    total_length = len(df) - (seq_len + pred_len - 1)\n",
    "    excess = total_length % batch_size\n",
    "    if excess > 0:\n",
    "        adjusted_length = len(df) - excess\n",
    "        df = df.iloc[:adjusted_length]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Metrics #################################################################################################################\n",
    "### ALL METRICS ARE INTERCHANGEABLE WITH INFORMER AUTOFORMER\n",
    "def RSE(pred, true):\n",
    "    '''\n",
    "    Calculates relative quared error.\n",
    "    '''\n",
    "    return np.sqrt(np.sum((true-pred)**2)) / np.sqrt(np.sum((true-true.mean())**2))\n",
    "\n",
    "def CORR(pred, true):\n",
    "    '''\n",
    "    Calculates correlation coefficient.\n",
    "    '''\n",
    "    u = ((true-true.mean(0))*(pred-pred.mean(0))).sum(0) \n",
    "    d = np.sqrt(((true-true.mean(0))**2*(pred-pred.mean(0))**2).sum(0))\n",
    "    return (u/d).mean(-1)\n",
    "\n",
    "def MAE(pred, true):\n",
    "    '''\n",
    "    Calculates mean absolute error.\n",
    "    '''\n",
    "    return np.mean(np.abs(pred-true))\n",
    "\n",
    "def MSE(pred, true):\n",
    "    '''\n",
    "    Calculates mean squared error.\n",
    "    '''\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "def RMSE(pred, true):\n",
    "    '''\n",
    "    Calculates root mean suared error.\n",
    "    '''\n",
    "    return np.sqrt(MSE(pred, true))\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    '''\n",
    "    Calculates mean absolute percentage error.\n",
    "    '''\n",
    "    return np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "def MSPE(pred, true):\n",
    "    '''\n",
    "    Calculates mean squared percentage error.\n",
    "    '''\n",
    "    return np.mean(np.square((pred - true) / true))\n",
    "\n",
    "def metric(pred, true):\n",
    "    '''\n",
    "    Wraps up metric functions, calculates and returns all.\n",
    "    '''\n",
    "    mae = MAE(pred, true)\n",
    "    mse = MSE(pred, true)\n",
    "    rmse = RMSE(pred, true)\n",
    "    mape = MAPE(pred, true)\n",
    "    mspe = MSPE(pred, true)\n",
    "    \n",
    "    return mae,mse,rmse,mape,mspe\n",
    "\n",
    "\n",
    "# Dot dictionary ##########################################################################################################\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "########## TODOOOO\n",
    "\n",
    "###########################################################################################################################\n",
    "#  Data loader and dependencies ###########################################################################################  \n",
    "class TSFDataLoader:\n",
    "    \"\"\"Generate data loader from raw data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, root_path, batch_size, seq_len, pred_len, data_path='SYNTHh.csv', features='S', target='TARGET'\n",
    "            ):\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.target_slice = slice(0, None)\n",
    "\n",
    "        self._read_data()\n",
    "\n",
    "    def _read_data(self):\n",
    "        \"\"\"Load raw data and split datasets.\"\"\"\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        # S: univariate-univariate, M: multivariate-multivariate, MS:\n",
    "        # multivariate-univariate\n",
    "        df = df_raw.set_index('date')\n",
    "        \n",
    "        if self.features == 'S':\n",
    "            df = df[[self.target]]\n",
    "        elif self.features == 'MS': ## TODO check how this functions with multivariate once we have it\n",
    "            target_idx = df.columns.get_loc(self.target)\n",
    "            self.target_slice = slice(target_idx, target_idx + 1)\n",
    "\n",
    "        # split train/valid/test\n",
    "        n = len(df)\n",
    "        # THE SPLITS below match the splits of Informer, Crossformer, Autoformer, Fedformer\n",
    "        if self.data_path.startswith('ETTh'): # keeping this here bc we wanna include ETTh\n",
    "            train_end = 12 * 30 * 24\n",
    "            val_end = train_end + 4 * 30 * 24\n",
    "            test_end = val_end + 4 * 30 * 24\n",
    "        # I added two more elifs for synth and wind data, we can do the split provision here too\n",
    "        elif self.data_path.startswith('SYNTHh'):\n",
    "            train_end = 12 * 30 * 24\n",
    "            val_end = train_end + 4 * 30 * 24\n",
    "            test_end = val_end + 4 * 30 * 24\n",
    "        elif self.data_path.startswith('DEWINDh'):\n",
    "            train_end = 12 * 30 * 24\n",
    "            val_end = train_end + 4 * 30 * 24\n",
    "            test_end = val_end + 4 * 30 * 24\n",
    "        else: # results to the good old train-val-test split by ratios\n",
    "            train_end = int(n * 0.7)\n",
    "            val_end = n - int(n * 0.2)\n",
    "            test_end = n\n",
    "\n",
    "        train_df = df[:train_end]\n",
    "        val_df = df[train_end - self.seq_len : val_end]\n",
    "        test_df = df[val_end - self.seq_len : test_end]\n",
    "       \n",
    "        # Drop last (if incomplete) batches\n",
    "        train_df = drop_last_for_tensorflow(train_df, self.batch_size, self.seq_len, self.pred_len)\n",
    "        val_df = drop_last_for_tensorflow(val_df, self.batch_size, self.seq_len, self.pred_len)\n",
    "        test_df = drop_last_for_tensorflow(test_df, self.batch_size, self.seq_len, self.pred_len)\n",
    "\n",
    "\n",
    "        # standardize by training set\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(train_df.values)\n",
    "\n",
    "        def scale_df(df, scaler):\n",
    "            data = scaler.transform(df.values)\n",
    "            return pd.DataFrame(data, index=df.index, columns=df.columns)\n",
    "\n",
    "        self.train_df = scale_df(train_df, self.scaler)\n",
    "        self.val_df = scale_df(val_df, self.scaler)\n",
    "        self.test_df = scale_df(test_df, self.scaler)\n",
    "        self.n_feature = self.train_df.shape[-1]\n",
    "\n",
    "    def _split_window(self, data):\n",
    "        inputs = data[:, : self.seq_len, :]\n",
    "        labels = data[:, self.seq_len :, self.target_slice]\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.seq_len, None])\n",
    "        labels.set_shape([None, self.pred_len, None])\n",
    "        return inputs, labels\n",
    "\n",
    "    def _make_dataset(self, data, shuffle=True):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=(self.seq_len + self.pred_len),\n",
    "            sequence_stride=1, # window stride\n",
    "            shuffle=shuffle,\n",
    "            batch_size=self.batch_size,\n",
    "            )\n",
    "        ds = ds.map(self._split_window)\n",
    "        return ds\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "    def get_train(self, shuffle=True):\n",
    "        return self._make_dataset(self.train_df, shuffle=shuffle)\n",
    "\n",
    "    def get_val(self):\n",
    "        return self._make_dataset(self.val_df, shuffle=False)\n",
    "\n",
    "    def get_test(self):\n",
    "        return self._make_dataset(self.test_df, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "# Reversible Instance Normalization #######################################################################################\n",
    "class RevNorm(layers.Layer):\n",
    "    \"\"\"Reversible Instance Normalization.\"\"\"\n",
    "\n",
    "    def __init__(self, axis, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.affine:\n",
    "            self.affine_weight = self.add_weight(\n",
    "               'affine_weight', shape=input_shape[-1], initializer='ones'\n",
    "               )\n",
    "            self.affine_bias = self.add_weight(\n",
    "               'affine_bias', shape=input_shape[-1], initializer='zeros'\n",
    "               )\n",
    "\n",
    "    def call(self, x, mode, target_slice=None):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x, target_slice)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        self.mean = tf.stop_gradient(\n",
    "           tf.reduce_mean(x, axis=self.axis, keepdims=True)\n",
    "           )\n",
    "        self.stdev = tf.stop_gradient(\n",
    "           tf.sqrt(\n",
    "              tf.math.reduce_variance(x, axis=self.axis, keepdims=True) + self.eps\n",
    "              )\n",
    "            )\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x, target_slice=None):\n",
    "        if self.affine:\n",
    "           x = x - self.affine_bias[target_slice]\n",
    "           x = x / self.affine_weight[target_slice]\n",
    "        x = x * self.stdev[:, :, target_slice]\n",
    "        x = x + self.mean[:, :, target_slice]\n",
    "        return x\n",
    "  \n",
    "\n",
    "###########################################################################################################################\n",
    "# TSMIxer Block ###########################################################################################################\n",
    "def res_block(inputs, norm_type, activation, dropout, ff_dim):\n",
    "    \"\"\"Residual block of TSMixer.\"\"\"\n",
    "\n",
    "    norm = (\n",
    "       layers.LayerNormalization if norm_type == 'L' else layers.BatchNormalization\n",
    "       )\n",
    "\n",
    "    # Temporal Linear\n",
    "    x = norm(axis=[-2, -1])(inputs)\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
    "    x = layers.Dense(x.shape[-1], activation=activation)(x)\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Input Length, Channel]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "  # Feature Linear\n",
    "    x = norm(axis=[-2, -1])(res)\n",
    "    x = layers.Dense(ff_dim, activation=activation)(\n",
    "       x\n",
    "    )  # [Batch, Input Length, FF_Dim]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    return x + res    \n",
    "  \n",
    "###########################################################################################################################\n",
    "# Build TSMixer with Reversible Instance Normalization ####################################################################\n",
    "def build_model(\n",
    "      input_shape,\n",
    "      pred_len,\n",
    "      norm_type,\n",
    "      activation,\n",
    "      n_block,\n",
    "      dropout,\n",
    "      ff_dim,\n",
    "      target_slice,\n",
    "    ):\n",
    "    \n",
    "    \"\"\"Build TSMixer with Reversible Instance Normalization model.\"\"\"\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs  # [Batch, Input Length, Channel]\n",
    "    rev_norm = RevNorm(axis=-2)\n",
    "    x = rev_norm(x, 'norm')\n",
    "    for _ in range(n_block):\n",
    "        x = res_block(x, norm_type, activation, dropout, ff_dim)\n",
    "\n",
    "    if target_slice:\n",
    "        x = x[:, :, target_slice]\n",
    "\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
    "    x = layers.Dense(pred_len)(x)  # [Batch, Channel, Output Length]\n",
    "    outputs = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Output Length, Channel])\n",
    "    outputs = rev_norm(outputs, 'denorm', target_slice)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "class TSMixer():\n",
    "    '''\n",
    "    I am so thoroughly exhausted you cannot imagine\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model='tsmixer_rev_in'):\n",
    "        self.args = dotdict()\n",
    "        self.args.model = model ## I keep these redundant model args to maybe then combine all API-s\n",
    "        self.args.seed = 0\n",
    "        \n",
    "        ## Variables for Multivariate ##################################################################\n",
    "        ## TODO ## \n",
    "        ## I need to take this outside in a method called get_data maybe?\n",
    "        ## Choices for this and other API-s must be ['S', 'M', 'MS] <-- check if Crossformer can handle\n",
    "        self.args.features = 'S' ## currently tailored to synth and wind series\n",
    "        self.args.target = 'TARGET' ## Because I give this name to the column\n",
    "        ################################################################################################\n",
    "        \n",
    "        \n",
    "        self.args.checkpoints = './tsmixer_checkpoints'\n",
    "        self.args.delete_checkpoint = False ## I am not sure this is correct default\n",
    "\n",
    "        ## Variables for TS\n",
    "        self.args.seq_len = 96 # used the default of other models, authors set it to 336\n",
    "\n",
    "        # Model Architecture\n",
    "        #self.kernel_size = 4 ## deactivated because we do not fit CNN\n",
    "        self.args.n_block = 2 ## number of blocks for deep architecture\n",
    "        self.args.ff_dim = 2048 ## fully-connected feature dimension\n",
    "        self.args.dropout = 0.05 ## dropout rate\n",
    "        self.args.norm_type = 'B' ## BatchNorm. Authors included alternative -- 'L' LayerNorm\n",
    "        self.args.activation = 'relu' ## Authors included possible alternative -- 'gelu'\n",
    "        self.args.temporal_dim = 16 ## temporal feature dimension\n",
    "        self.args.hidden_dim = 64 ## hidden feature dimension\n",
    "        self.args.num_workers = 19 ## maybe switch this to 0 like other models if there is a problem\n",
    "\n",
    "\n",
    "\n",
    "        # Add root_path, data_path as args. \n",
    "        # root path serves as LOCAL_CACHE_DIR\n",
    "        # data_path serves as data + '.csv'\n",
    "\n",
    "\n",
    "    def compile(self, learning_rate=1e-4, loss='mse', early_stopping_patience=5):\n",
    "        ## should include\n",
    "        ## loss, \n",
    "        if loss != 'mse':\n",
    "            raise ValueError(\"Loss function not supported. Please use 'mse'.\")\n",
    "        self.args.loss = loss\n",
    "        self.args.learning_rate = learning_rate\n",
    "        self.args.patience = early_stopping_patience\n",
    "\n",
    "    \n",
    "    def fit(self, data='SYNTHh', data_root_path='./SynthDataset/', batch_size=32, epochs=100, pred_len=24):\n",
    "        ## Should include\n",
    "        ## data, data_root_path, batch_size, epochs, pred_len\n",
    "        possible_predlens = [24, 48, 96, 168, 336, 720]\n",
    "        if pred_len not in possible_predlens:\n",
    "            raise ValueError('Prediction length outside current experiment scope. Please use either 24, 48, 96, 168, 336, 720.')\n",
    "        self.args.data = data ## NOTE this is redundant because the self.args.data in the other wrappers is used because it is needed for data_parser. Here parsing happens inside the data loader.\n",
    "        self.args.root_path = data_root_path\n",
    "        self.args.data_path = f'{self.args.data}.csv'\n",
    "        self.args.pred_len = pred_len\n",
    "        self.args.batch_size = batch_size ## 32 is the authors' default\n",
    "        self.args.train_epochs = epochs ## 100 is the authors' default\n",
    "\n",
    "        print('Beginning to fit the model with the following arguments:')\n",
    "        print(f'{self.args}')\n",
    "        print('='*150)  \n",
    "\n",
    "        self.setting = f'TSMixer_{self.args.data}_{self.args.features}_sl{self.args.seq_len}_pl{self.args.pred_len}_lr{self.args.learning_rate}_nt{self.args.norm_type}_{self.args.activation}_nb{self.args.n_block}_dp{self.args.dropout}_fd{self.args.ff_dim}'\n",
    "        \n",
    "        tf.keras.utils.set_random_seed(self.args.seed)\n",
    "        \n",
    "        # Initialize the data loader\n",
    "        data_loader = TSFDataLoader(\n",
    "            root_path=self.args.root_path,\n",
    "            batch_size=self.args.batch_size,\n",
    "            seq_len=self.args.seq_len,\n",
    "            pred_len=self.args.pred_len,\n",
    "            data_path=self.args.data_path,\n",
    "            features=self.args.features,\n",
    "            target=self.args.target,\n",
    "        )\n",
    "        # Load train, val, test data\n",
    "        self.train_data = data_loader.get_train()\n",
    "        self.val_data = data_loader.get_val()\n",
    "        self.test_data = data_loader.get_test()\n",
    "        # Build model\n",
    "        model = build_model(\n",
    "            input_shape=(self.args.seq_len, data_loader.n_feature),\n",
    "            pred_len=self.args.pred_len,\n",
    "            norm_type=self.args.norm_type,\n",
    "            activation=self.args.activation,\n",
    "            dropout=self.args.dropout,\n",
    "            n_block=self.args.n_block,\n",
    "            ff_dim=self.args.ff_dim,\n",
    "            target_slice=data_loader.target_slice,\n",
    "        )\n",
    "        \n",
    "        # Set up optimizer\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.args.learning_rate)\n",
    "        # True compilation\n",
    "        model.compile(optimizer=optimizer, loss=self.args.loss, metrics=['mae'])\n",
    "        checkpoint_path = os.path.join(self.args.checkpoints, f'{self.setting}_best')\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "        )\n",
    "        early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=self.args.patience\n",
    "        )\n",
    "        start_training_time = time.time()\n",
    "        \n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "            self.train_data,\n",
    "            epochs=self.args.train_epochs,\n",
    "            validation_data=self.val_data,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            )\n",
    "        end_training_time = time.time()\n",
    "        elasped_training_time = end_training_time - start_training_time\n",
    "        print(f'Training finished in {elasped_training_time} secconds')\n",
    "\n",
    "        # evaluate best model on the val set\n",
    "        # Load weights from the checkpoint\n",
    "        best_epoch = np.argmin(history.history['val_loss'])\n",
    "        model.load_weights(checkpoint_path)\n",
    "        self.model = model # Save as self to move on to .predict()\n",
    "\n",
    "        #return self.model ## NOTE why are we not returning the best model?\n",
    "    \n",
    "    def predict(self):\n",
    "        # Generate predictions\n",
    "        self.preds = self.model.predict(self.test_data, batch_size=self.args.batch_size)\n",
    "\n",
    "        # Extract y_trues from DataLoader\n",
    "        trues_list = []\n",
    "\n",
    "        # Iterate over the dataset to collect the targets\n",
    "        for _, targets in self.test_data:\n",
    "            # Convert the targets to numpy and store\n",
    "            trues_list.append(targets.numpy())\n",
    "\n",
    "        # Concatenate the list of targets into a single numpy array\n",
    "        self.trues = np.concatenate(trues_list, axis=0)\n",
    "                \n",
    "        if self.args.delete_checkpoint:\n",
    "            for f in glob.glob(self.args.checkpoint_path + '*'):\n",
    "                os.remove(f)\n",
    "\n",
    "        return self.preds, self.trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_mixer = TSMixer(model='tsmixer_rev_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_mixer.compile(learning_rate=0.0001, loss='mse', early_stopping_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to fit the model with the following arguments:\n",
      "{'model': 'tsmixer_rev_in', 'seed': 0, 'features': 'S', 'target': 'TARGET', 'checkpoints': './tsmixer_checkpoints', 'delete_checkpoint': False, 'seq_len': 96, 'n_block': 2, 'ff_dim': 2048, 'dropout': 0.05, 'norm_type': 'B', 'activation': 'relu', 'temporal_dim': 16, 'hidden_dim': 64, 'num_workers': 19, 'loss': 'mse', 'learning_rate': 0.0001, 'patience': 5, 'data': 'SYNTHh', 'root_path': './SynthDataset/', 'data_path': 'SYNTHh.csv', 'pred_len': 24, 'batch_size': 32, 'train_epochs': 1}\n",
      "======================================================================================================================================================\n",
      "TSMixer_SYNTHh_S_sl96_pl24_lr0.0001_ntB_relu_nb2_dp0.05_fd2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 18:05:12.266480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 336 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-02-04 18:05:12.277133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1229 MB memory:  -> device: 1, name: NVIDIA A40, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-02-04 18:05:23.906791: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f4d28639fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-04 18:05:23.906855: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\n",
      "2024-02-04 18:05:23.906866: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A40, Compute Capability 8.6\n",
      "2024-02-04 18:05:23.912686: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-04 18:05:23.954407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-02-04 18:05:24.223368: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265/266 [============================>.] - ETA: 0s - loss: 1.3378 - mae: 0.9002\n",
      "Epoch 1: val_loss improved from inf to 0.60354, saving model to ./tsmixer_checkpoints/TSMixer_SYNTHh_S_sl96_pl24_lr0.0001_ntB_relu_nb2_dp0.05_fd2048_best\n",
      "266/266 [==============================] - 24s 48ms/step - loss: 1.3352 - mae: 0.8993 - val_loss: 0.6035 - val_mae: 0.6327\n",
      "Training finished in 44.43026566505432 secconds\n"
     ]
    }
   ],
   "source": [
    "ts_mixer.fit(\n",
    "    data='SYNTHh',\n",
    "    data_root_path='./SynthDataset/',\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    pred_len=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 2s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "y_preds, y_trues = ts_mixer.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2848, 24, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InformerAPI import InformerTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_informer = InformerTS('informerstack')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anbs_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
