{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from math import sqrt, ceil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 18:26:45.961425: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-04 18:26:46.031696: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-04 18:26:46.031739: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-04 18:26:46.031768: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-04 18:26:46.049380: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "### TSMixer API\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "# NOTE maybe I should take this outside\n",
    "def drop_last_for_tensorflow(df, batch_size, seq_len, pred_len):\n",
    "    '''\n",
    "    Emulates PyTorch dataloaders' option for drop_last = True\n",
    "    '''\n",
    "    total_length = len(df) - (seq_len + pred_len - 1)\n",
    "    excess = total_length % batch_size\n",
    "    if excess > 0:\n",
    "        adjusted_length = len(df) - excess\n",
    "        df = df.iloc[:adjusted_length]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Metrics #################################################################################################################\n",
    "### ALL METRICS ARE INTERCHANGEABLE WITH INFORMER AUTOFORMER\n",
    "def RSE(pred, true):\n",
    "    '''\n",
    "    Calculates relative quared error.\n",
    "    '''\n",
    "    return np.sqrt(np.sum((true-pred)**2)) / np.sqrt(np.sum((true-true.mean())**2))\n",
    "\n",
    "def CORR(pred, true):\n",
    "    '''\n",
    "    Calculates correlation coefficient.\n",
    "    '''\n",
    "    u = ((true-true.mean(0))*(pred-pred.mean(0))).sum(0) \n",
    "    d = np.sqrt(((true-true.mean(0))**2*(pred-pred.mean(0))**2).sum(0))\n",
    "    return (u/d).mean(-1)\n",
    "\n",
    "def MAE(pred, true):\n",
    "    '''\n",
    "    Calculates mean absolute error.\n",
    "    '''\n",
    "    return np.mean(np.abs(pred-true))\n",
    "\n",
    "def MSE(pred, true):\n",
    "    '''\n",
    "    Calculates mean squared error.\n",
    "    '''\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "def RMSE(pred, true):\n",
    "    '''\n",
    "    Calculates root mean suared error.\n",
    "    '''\n",
    "    return np.sqrt(MSE(pred, true))\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    '''\n",
    "    Calculates mean absolute percentage error.\n",
    "    '''\n",
    "    return np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "def MSPE(pred, true):\n",
    "    '''\n",
    "    Calculates mean squared percentage error.\n",
    "    '''\n",
    "    return np.mean(np.square((pred - true) / true))\n",
    "\n",
    "def metric(pred, true):\n",
    "    '''\n",
    "    Wraps up metric functions, calculates and returns all.\n",
    "    '''\n",
    "    mae = MAE(pred, true)\n",
    "    mse = MSE(pred, true)\n",
    "    rmse = RMSE(pred, true)\n",
    "    mape = MAPE(pred, true)\n",
    "    mspe = MSPE(pred, true)\n",
    "    \n",
    "    return mae,mse,rmse,mape,mspe\n",
    "\n",
    "\n",
    "# Dot dictionary ##########################################################################################################\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "########## TODOOOO\n",
    "\n",
    "###########################################################################################################################\n",
    "#  Data loader and dependencies ###########################################################################################  \n",
    "class TSFDataLoader:\n",
    "    \"\"\"Generate data loader from raw data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, root_path, batch_size, seq_len, pred_len, data_path='SYNTHh.csv', features='S', target='TARGET'\n",
    "            ):\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.target_slice = slice(0, None)\n",
    "\n",
    "        self._read_data()\n",
    "\n",
    "    def _read_data(self):\n",
    "        \"\"\"Load raw data and split datasets.\"\"\"\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        # S: univariate-univariate, M: multivariate-multivariate, MS:\n",
    "        # multivariate-univariate\n",
    "        df = df_raw.set_index('date')\n",
    "        \n",
    "        if self.features == 'S':\n",
    "            df = df[[self.target]]\n",
    "        elif self.features == 'MS': ## TODO check how this functions with multivariate once we have it\n",
    "            target_idx = df.columns.get_loc(self.target)\n",
    "            self.target_slice = slice(target_idx, target_idx + 1)\n",
    "\n",
    "        # split train/valid/test\n",
    "        n = len(df)\n",
    "        # THE SPLITS below match the splits of Informer, Crossformer, Autoformer, Fedformer\n",
    "        if self.data_path.startswith('ETTh'): # keeping this here bc we wanna include ETTh\n",
    "            train_end = 12 * 30 * 24\n",
    "            val_end = train_end + 4 * 30 * 24\n",
    "            test_end = val_end + 4 * 30 * 24\n",
    "        # I added two more elifs for synth and wind data, we can do the split provision here too\n",
    "        elif self.data_path.startswith('SYNTHh'):\n",
    "            train_end = 12 * 30 * 24\n",
    "            val_end = train_end + 4 * 30 * 24\n",
    "            test_end = val_end + 4 * 30 * 24\n",
    "        elif self.data_path.startswith('DEWINDh'):\n",
    "            train_end = 12 * 30 * 24\n",
    "            val_end = train_end + 4 * 30 * 24\n",
    "            test_end = val_end + 4 * 30 * 24\n",
    "        else: # results to the good old train-val-test split by ratios\n",
    "            train_end = int(n * 0.7)\n",
    "            val_end = n - int(n * 0.2)\n",
    "            test_end = n\n",
    "\n",
    "        train_df = df[:train_end]\n",
    "        val_df = df[train_end - self.seq_len : val_end]\n",
    "        test_df = df[val_end - self.seq_len : test_end]\n",
    "       \n",
    "        # Drop last (if incomplete) batches\n",
    "        train_df = drop_last_for_tensorflow(train_df, self.batch_size, self.seq_len, self.pred_len)\n",
    "        val_df = drop_last_for_tensorflow(val_df, self.batch_size, self.seq_len, self.pred_len)\n",
    "        test_df = drop_last_for_tensorflow(test_df, self.batch_size, self.seq_len, self.pred_len)\n",
    "        \n",
    "        # standardize by training set\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(train_df.values)\n",
    "\n",
    "        def scale_df(df, scaler):\n",
    "            data = scaler.transform(df.values)\n",
    "            return pd.DataFrame(data, index=df.index, columns=df.columns)\n",
    "\n",
    "        self.train_df = scale_df(train_df, self.scaler)\n",
    "        self.val_df = scale_df(val_df, self.scaler)\n",
    "        self.test_df = scale_df(test_df, self.scaler)\n",
    "        self.n_feature = self.train_df.shape[-1]\n",
    "\n",
    "    def _split_window(self, data):\n",
    "        inputs = data[:, : self.seq_len, :]\n",
    "        labels = data[:, self.seq_len :, self.target_slice]\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.seq_len, None])\n",
    "        labels.set_shape([None, self.pred_len, None])\n",
    "        return inputs, labels\n",
    "\n",
    "    def _make_dataset(self, data, shuffle=True):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=(self.seq_len + self.pred_len),\n",
    "            sequence_stride=1, # window stride\n",
    "            shuffle=shuffle,\n",
    "            batch_size=self.batch_size,\n",
    "            )\n",
    "        ds = ds.map(self._split_window)\n",
    "        return ds\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "    def get_train(self, shuffle=True):\n",
    "        return self._make_dataset(self.train_df, shuffle=shuffle)\n",
    "\n",
    "    def get_val(self):\n",
    "        return self._make_dataset(self.val_df, shuffle=False)\n",
    "\n",
    "    def get_test(self):\n",
    "        return self._make_dataset(self.test_df, shuffle=False)\n",
    "\n",
    "\n",
    "###########################################################################################################################\n",
    "# Reversible Instance Normalization #######################################################################################\n",
    "class RevNorm(layers.Layer):\n",
    "    \"\"\"Reversible Instance Normalization.\"\"\"\n",
    "\n",
    "    def __init__(self, axis, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.affine:\n",
    "            self.affine_weight = self.add_weight(\n",
    "               'affine_weight', shape=input_shape[-1], initializer='ones'\n",
    "               )\n",
    "            self.affine_bias = self.add_weight(\n",
    "               'affine_bias', shape=input_shape[-1], initializer='zeros'\n",
    "               )\n",
    "\n",
    "    def call(self, x, mode, target_slice=None):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x, target_slice)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        self.mean = tf.stop_gradient(\n",
    "           tf.reduce_mean(x, axis=self.axis, keepdims=True)\n",
    "           )\n",
    "        self.stdev = tf.stop_gradient(\n",
    "           tf.sqrt(\n",
    "              tf.math.reduce_variance(x, axis=self.axis, keepdims=True) + self.eps\n",
    "              )\n",
    "            )\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x, target_slice=None):\n",
    "        if self.affine:\n",
    "           x = x - self.affine_bias[target_slice]\n",
    "           x = x / self.affine_weight[target_slice]\n",
    "        x = x * self.stdev[:, :, target_slice]\n",
    "        x = x + self.mean[:, :, target_slice]\n",
    "        return x\n",
    "  \n",
    "\n",
    "###########################################################################################################################\n",
    "# TSMIxer Block ###########################################################################################################\n",
    "def res_block(inputs, norm_type, activation, dropout, ff_dim):\n",
    "    \"\"\"Residual block of TSMixer.\"\"\"\n",
    "\n",
    "    norm = (\n",
    "       layers.LayerNormalization if norm_type == 'L' else layers.BatchNormalization\n",
    "       )\n",
    "\n",
    "    # Temporal Linear\n",
    "    x = norm(axis=[-2, -1])(inputs)\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
    "    x = layers.Dense(x.shape[-1], activation=activation)(x)\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Input Length, Channel]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "  # Feature Linear\n",
    "    x = norm(axis=[-2, -1])(res)\n",
    "    x = layers.Dense(ff_dim, activation=activation)(\n",
    "       x\n",
    "    )  # [Batch, Input Length, FF_Dim]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    return x + res    \n",
    "  \n",
    "###########################################################################################################################\n",
    "# Build TSMixer with Reversible Instance Normalization ####################################################################\n",
    "def build_model(\n",
    "      input_shape,\n",
    "      pred_len,\n",
    "      norm_type,\n",
    "      activation,\n",
    "      n_block,\n",
    "      dropout,\n",
    "      ff_dim,\n",
    "      target_slice,\n",
    "    ):\n",
    "    \n",
    "    \"\"\"Build TSMixer with Reversible Instance Normalization model.\"\"\"\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs  # [Batch, Input Length, Channel]\n",
    "    rev_norm = RevNorm(axis=-2)\n",
    "    x = rev_norm(x, 'norm')\n",
    "    for _ in range(n_block):\n",
    "        x = res_block(x, norm_type, activation, dropout, ff_dim)\n",
    "\n",
    "    if target_slice:\n",
    "        x = x[:, :, target_slice]\n",
    "\n",
    "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
    "    x = layers.Dense(pred_len)(x)  # [Batch, Channel, Output Length]\n",
    "    outputs = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Output Length, Channel])\n",
    "    outputs = rev_norm(outputs, 'denorm', target_slice)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "class TSMixer():\n",
    "    '''\n",
    "    I am so thoroughly exhausted you cannot imagine\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model='tsmixer_rev_in'):\n",
    "        self.args = dotdict()\n",
    "        self.args.model = model ## I keep these redundant model args to maybe then combine all API-s\n",
    "        self.args.seed = 0\n",
    "        \n",
    "        ## Variables for Multivariate ##################################################################\n",
    "        ## TODO ## \n",
    "        ## I need to take this outside in a method called get_data maybe?\n",
    "        ## Choices for this and other API-s must be ['S', 'M', 'MS] <-- check if Crossformer can handle\n",
    "        self.args.features = 'S' ## currently tailored to synth and wind series\n",
    "        self.args.target = 'TARGET' ## Because I give this name to the column\n",
    "        ################################################################################################\n",
    "        \n",
    "        \n",
    "        self.args.checkpoints = './tsmixer_checkpoints'\n",
    "        self.args.delete_checkpoint = False ## I am not sure this is correct default\n",
    "\n",
    "        ## Variables for TS\n",
    "        self.args.seq_len = 96 # used the default of other models, authors set it to 336\n",
    "\n",
    "        # Model Architecture\n",
    "        #self.kernel_size = 4 ## deactivated because we do not fit CNN\n",
    "        self.args.n_block = 2 ## number of blocks for deep architecture\n",
    "        self.args.ff_dim = 2048 ## fully-connected feature dimension\n",
    "        self.args.dropout = 0.05 ## dropout rate\n",
    "        self.args.norm_type = 'B' ## BatchNorm. Authors included alternative -- 'L' LayerNorm\n",
    "        self.args.activation = 'relu' ## Authors included possible alternative -- 'gelu'\n",
    "        self.args.temporal_dim = 16 ## temporal feature dimension\n",
    "        self.args.hidden_dim = 64 ## hidden feature dimension\n",
    "        self.args.num_workers = 19 ## maybe switch this to 0 like other models if there is a problem\n",
    "\n",
    "\n",
    "\n",
    "        # Add root_path, data_path as args. \n",
    "        # root path serves as LOCAL_CACHE_DIR\n",
    "        # data_path serves as data + '.csv'\n",
    "\n",
    "\n",
    "    def compile(self, learning_rate=1e-4, loss='mse', early_stopping_patience=5):\n",
    "        ## should include\n",
    "        ## loss, \n",
    "        if loss != 'mse':\n",
    "            raise ValueError(\"Loss function not supported. Please use 'mse'.\")\n",
    "        self.args.loss = loss\n",
    "        self.args.learning_rate = learning_rate\n",
    "        self.args.patience = early_stopping_patience\n",
    "\n",
    "    \n",
    "    def fit(self, data='SYNTHh', data_root_path='./SynthDataset/', batch_size=32, epochs=100, pred_len=24):\n",
    "        ## Should include\n",
    "        ## data, data_root_path, batch_size, epochs, pred_len\n",
    "        possible_predlens = [24, 48, 96, 168, 336, 720]\n",
    "        if pred_len not in possible_predlens:\n",
    "            raise ValueError('Prediction length outside current experiment scope. Please use either 24, 48, 96, 168, 336, 720.')\n",
    "        self.args.data = data ## NOTE this is redundant because the self.args.data in the other wrappers is used because it is needed for data_parser. Here parsing happens inside the data loader.\n",
    "        self.args.root_path = data_root_path\n",
    "        self.args.data_path = f'{self.args.data}.csv'\n",
    "        self.args.pred_len = pred_len\n",
    "        self.args.batch_size = batch_size ## 32 is the authors' default\n",
    "        self.args.train_epochs = epochs ## 100 is the authors' default\n",
    "\n",
    "        print('Beginning to fit the model with the following arguments:')\n",
    "        print(f'{self.args}')\n",
    "        print('='*150)  \n",
    "\n",
    "        self.setting = f'TSMixer_{self.args.data}_{self.args.features}_sl{self.args.seq_len}_pl{self.args.pred_len}_lr{self.args.learning_rate}_nt{self.args.norm_type}_{self.args.activation}_nb{self.args.n_block}_dp{self.args.dropout}_fd{self.args.ff_dim}'\n",
    "        \n",
    "        tf.keras.utils.set_random_seed(self.args.seed)\n",
    "        \n",
    "        # Initialize the data loader\n",
    "        data_loader = TSFDataLoader(\n",
    "            root_path=self.args.root_path,\n",
    "            batch_size=self.args.batch_size,\n",
    "            seq_len=self.args.seq_len,\n",
    "            pred_len=self.args.pred_len,\n",
    "            data_path=self.args.data_path,\n",
    "            features=self.args.features,\n",
    "            target=self.args.target,\n",
    "        )\n",
    "        # Load train, val, test data\n",
    "        self.train_data = data_loader.get_train()\n",
    "        self.val_data = data_loader.get_val()\n",
    "        self.test_data = data_loader.get_test()\n",
    "        # Build model\n",
    "        model = build_model(\n",
    "            input_shape=(self.args.seq_len, data_loader.n_feature),\n",
    "            pred_len=self.args.pred_len,\n",
    "            norm_type=self.args.norm_type,\n",
    "            activation=self.args.activation,\n",
    "            dropout=self.args.dropout,\n",
    "            n_block=self.args.n_block,\n",
    "            ff_dim=self.args.ff_dim,\n",
    "            target_slice=data_loader.target_slice,\n",
    "        )\n",
    "        \n",
    "        # Set up optimizer\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.args.learning_rate)\n",
    "        # True compilation\n",
    "        model.compile(optimizer=optimizer, loss=self.args.loss, metrics=['mae'])\n",
    "        checkpoint_path = os.path.join(self.args.checkpoints, f'{self.setting}_best')\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "        )\n",
    "        early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=self.args.patience\n",
    "        )\n",
    "        start_training_time = time.time()\n",
    "        \n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "            self.train_data,\n",
    "            epochs=self.args.train_epochs,\n",
    "            validation_data=self.val_data,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            )\n",
    "        end_training_time = time.time()\n",
    "        elasped_training_time = end_training_time - start_training_time\n",
    "        print(f'Training finished in {elasped_training_time} secconds')\n",
    "\n",
    "        # evaluate best model on the val set\n",
    "        # Load weights from the checkpoint\n",
    "        best_epoch = np.argmin(history.history['val_loss'])\n",
    "        model.load_weights(checkpoint_path)\n",
    "        self.model = model # Save as self to move on to .predict()\n",
    "\n",
    "        #return self.model ## NOTE why are we not returning the best model?\n",
    "    \n",
    "    def predict(self):\n",
    "        # Generate predictions\n",
    "        self.preds = self.model.predict(self.test_data, batch_size=self.args.batch_size)\n",
    "\n",
    "        # Extract y_trues from DataLoader\n",
    "        trues_list = []\n",
    "\n",
    "        # Iterate over the dataset to collect the targets\n",
    "        for _, targets in self.test_data:\n",
    "            # Convert the targets to numpy and store\n",
    "            trues_list.append(targets.numpy())\n",
    "\n",
    "        # Concatenate the list of targets into a single numpy array\n",
    "        self.trues = np.concatenate(trues_list, axis=0)\n",
    "                \n",
    "        if self.args.delete_checkpoint:\n",
    "            for f in glob.glob(self.args.checkpoint_path + '*'):\n",
    "                os.remove(f)\n",
    "\n",
    "        return self.preds, self.trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_mixer = TSMixer(model='tsmixer_rev_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_mixer.compile(learning_rate=0.0001, loss='mse', early_stopping_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to fit the model with the following arguments:\n",
      "{'model': 'tsmixer_rev_in', 'seed': 0, 'features': 'S', 'target': 'TARGET', 'checkpoints': './tsmixer_checkpoints', 'delete_checkpoint': False, 'seq_len': 96, 'n_block': 2, 'ff_dim': 2048, 'dropout': 0.05, 'norm_type': 'B', 'activation': 'relu', 'temporal_dim': 16, 'hidden_dim': 64, 'num_workers': 19, 'loss': 'mse', 'learning_rate': 0.0001, 'patience': 5, 'data': 'SYNTHh', 'root_path': './SynthDataset/', 'data_path': 'SYNTHh.csv', 'pred_len': 24, 'batch_size': 32, 'train_epochs': 1}\n",
      "======================================================================================================================================================\n",
      "train 8631\n",
      "val 2967\n",
      "test 2967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 18:26:53.037269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 49 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2024-02-04 18:26:53.050659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1229 MB memory:  -> device: 1, name: NVIDIA A40, pci bus id: 0000:65:00.0, compute capability: 8.6\n",
      "2024-02-04 18:26:53.141073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 49.69MiB (52101120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2024-02-04 18:27:04.571842: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:188] failed to create cublas handle: the resource allocation failed\n",
      "2024-02-04 18:27:04.572630: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:191] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2024-02-04 18:27:04.572700: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at matmul_op_impl.h:804 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node model/dense/Tensordot/MatMul defined at (most recent call last):\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/tmp/ipykernel_9185/903678194.py\", line 1, in <module>\n\n  File \"/tmp/ipykernel_9185/4030070293.py\", line 449, in fit\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/layers/core/dense.py\", line 244, in call\n\nAttempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node model/dense/Tensordot/MatMul}}]] [Op:__inference_train_function_4673]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mts_mixer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSYNTHh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_root_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./SynthDataset/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 449\u001b[0m, in \u001b[0;36mTSMixer.fit\u001b[0;34m(self, data, data_root_path, batch_size, epochs, pred_len)\u001b[0m\n\u001b[1;32m    446\u001b[0m start_training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m end_training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    456\u001b[0m elasped_training_time \u001b[38;5;241m=\u001b[39m end_training_time \u001b[38;5;241m-\u001b[39m start_training_time\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node model/dense/Tensordot/MatMul defined at (most recent call last):\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n\n  File \"/tmp/ipykernel_9185/903678194.py\", line 1, in <module>\n\n  File \"/tmp/ipykernel_9185/4030070293.py\", line 449, in fit\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/home/RDC/anasashb/.conda/envs/ansb/lib/python3.9/site-packages/keras/src/layers/core/dense.py\", line 244, in call\n\nAttempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node model/dense/Tensordot/MatMul}}]] [Op:__inference_train_function_4673]"
     ]
    }
   ],
   "source": [
    "ts_mixer.fit(\n",
    "    data='SYNTHh',\n",
    "    data_root_path='./SynthDataset/',\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    pred_len=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 2s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "y_preds, y_trues = ts_mixer.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_tsmixer.pkl', 'wb') as f:\n",
    "    pickle.dump(y_trues, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2848, 24, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InformerAPI import InformerTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_informer = InformerTS('informerstack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_informer.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to fit the model with the following arguments:\n",
      "{'model': 'informerstack', 'features': 'S', 'target': 'TARGET', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'enc_in': 1, 'dec_in': 1, 'c_out': 1, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'data': 'SYNTHh', 'root_path': './SynthDataset/', 'data_path': 'SYNTHh.csv', 'train_epochs': 1, 'batch_size': 32, 'pred_len': 24, 'detail_freq': 'h', 'learning_rate': 0.0001, 'loss': 'mse', 'patience': 3}\n",
      "======================================================================================================================================================\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_SYNTHh_ftS_sl96_ll48_pl24_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 44.35 GiB of which 49.69 MiB is free. Process 12384 has 42.90 GiB memory in use. Process 14993 has 638.00 MiB memory in use. Process 17851 has 772.00 MiB memory in use. Of the allocated memory 433.07 MiB is allocated by PyTorch, and 12.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mts_informer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSYNTHh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_root_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./SynthDataset/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1986\u001b[0m, in \u001b[0;36mInformerTS.fit\u001b[0;34m(self, data, data_root_path, batch_size, epochs, pred_len)\u001b[0m\n\u001b[1;32m   1984\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetting))\n\u001b[0;32m-> 1986\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1687\u001b[0m, in \u001b[0;36mExp_Informer.train\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m   1684\u001b[0m iter_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1686\u001b[0m model_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m-> 1687\u001b[0m pred, true \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_one_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1689\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, true)\n\u001b[1;32m   1691\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1836\u001b[0m, in \u001b[0;36mExp_Informer._process_one_batch\u001b[0;34m(self, dataset_object, batch_x, batch_y, batch_x_mark, batch_y_mark)\u001b[0m\n\u001b[1;32m   1834\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x, batch_x_mark, dec_inp, batch_y_mark)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1836\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minverse:\n\u001b[1;32m   1838\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m dataset_object\u001b[38;5;241m.\u001b[39minverse_transform(outputs)\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1439\u001b[0m, in \u001b[0;36mInformerStack.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, \n\u001b[1;32m   1437\u001b[0m             enc_self_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dec_self_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dec_enc_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1438\u001b[0m     enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_embedding(x_enc, x_mark_enc)\n\u001b[0;32m-> 1439\u001b[0m     enc_out, attns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_self_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_embedding(x_dec, x_mark_dec)\n\u001b[1;32m   1442\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(dec_out, enc_out, x_mask\u001b[38;5;241m=\u001b[39mdec_self_mask, cross_mask\u001b[38;5;241m=\u001b[39mdec_enc_mask)\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1306\u001b[0m, in \u001b[0;36mEncoderStack.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_len, encoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_lens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders):\n\u001b[1;32m   1305\u001b[0m     inp_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi_len)\n\u001b[0;32m-> 1306\u001b[0m     x_s, attn \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43minp_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m     x_stack\u001b[38;5;241m.\u001b[39mappend(x_s); attns\u001b[38;5;241m.\u001b[39mappend(attn)\n\u001b[1;32m   1308\u001b[0m x_stack \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(x_stack, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1275\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attn_layer, conv_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers):\n\u001b[0;32m-> 1275\u001b[0m         x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         x \u001b[38;5;241m=\u001b[39m conv_layer(x)\n\u001b[1;32m   1277\u001b[0m         attns\u001b[38;5;241m.\u001b[39mappend(attn)\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1246\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m# x [B, L, D]\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;66;03m# x = x + self.dropout(self.attention(\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;66;03m#     x, x, x,\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;66;03m#     attn_mask = attn_mask\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;66;03m# ))\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m     new_x, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(new_x)\n\u001b[1;32m   1252\u001b[0m     y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1185\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m   1182\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_projection(keys)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1183\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_projection(values)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1185\u001b[0m out, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmix:\n\u001b[1;32m   1192\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ansb/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1107\u001b[0m, in \u001b[0;36mProbAttention.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m   1104\u001b[0m U_part \u001b[38;5;241m=\u001b[39m U_part \u001b[38;5;28;01mif\u001b[39;00m U_part\u001b[38;5;241m<\u001b[39mL_K \u001b[38;5;28;01melse\u001b[39;00m L_K\n\u001b[1;32m   1105\u001b[0m u \u001b[38;5;241m=\u001b[39m u \u001b[38;5;28;01mif\u001b[39;00m u\u001b[38;5;241m<\u001b[39mL_Q \u001b[38;5;28;01melse\u001b[39;00m L_Q\n\u001b[0;32m-> 1107\u001b[0m scores_top, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prob_QK\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mU_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m   1109\u001b[0m \u001b[38;5;66;03m# add scale factor\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39msqrt(D)\n",
      "File \u001b[0;32m~/Dokumente/TransformersEnergyTS/InformerAPI.py:1048\u001b[0m, in \u001b[0;36mProbAttention._prob_QK\u001b[0;34m(self, Q, K, sample_k, n_top)\u001b[0m\n\u001b[1;32m   1046\u001b[0m K_expand \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(B, H, L_Q, L_K, E)\n\u001b[1;32m   1047\u001b[0m index_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(L_K, (L_Q, sample_k)) \u001b[38;5;66;03m# real U = U_part(factor*ln(L_k))*L_q\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m K_sample \u001b[38;5;241m=\u001b[39m \u001b[43mK_expand\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL_Q\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1049\u001b[0m Q_K_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(Q\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), K_sample\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# find the Top_k query with sparisty measurement\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 44.35 GiB of which 49.69 MiB is free. Process 12384 has 42.90 GiB memory in use. Process 14993 has 638.00 MiB memory in use. Process 17851 has 772.00 MiB memory in use. Of the allocated memory 433.07 MiB is allocated by PyTorch, and 12.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "ts_informer.fit(\n",
    "    data='SYNTHh',\n",
    "    data_root_path='./SynthDataset/',\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    pred_len=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anbs_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
