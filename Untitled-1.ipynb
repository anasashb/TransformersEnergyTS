{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('seaborn-v0_8')\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# MSE function \n",
    "def MSE(pred, true):\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "random_seed = 100\n",
    "torch.manual_seed = 100\n",
    "np.random.seed = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 17:01:57.753643: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-19 17:01:57.825723: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 17:01:59.000833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from InformerAPI import InformerTS\n",
    "from CrossformerAPI import CrossformerTS\n",
    "from AutoformerAPI import AutoformerTS\n",
    "from FedformerAPI import FedformerTS\n",
    "from PyraformerAPI import PyraformerTS\n",
    "from LogSparseAPI import LogsparseTS\n",
    "from TSMixerAPI import TSMixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'ETTh1': ['./ETTDataset/' , 'M', 96] ,\n",
    "    #'DEWINDh_small': ['./WINDataset/' , 'S', 168] ,\n",
    "    #'SYNTHh1': ['./SYNTHDataset/', 'S', 168] ,\n",
    "    #'SYNTH_additive' : ['./SYNTHDataset/', 'S', 168] ,\n",
    "    #'SYNTH_additive_reversal' : ['./SYNTHDataset/', 'S', 168] ,\n",
    "    #'SYNTH_multiplicative' : ['./SYNTHDataset/', 'S', 168] ,\n",
    "    #'SYNTH_multiplicative_reversal' : ['./SYNTHDataset/' , 'S', 168]\n",
    "}\n",
    "pred_lens = [24, 48]\n",
    "\n",
    "models = [InformerTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model in use: InformerTS\n",
      "Training InformerTS on ETTh1 with pred_len 24\n",
      "Beginning to fit the model with the following arguments:\n",
      "{'model': 'informerstack', 'features': 'M', 'target': 'OT', 'freq': 'h', 'checkpoints': './checkpoints', 'seq_len': 96, 'label_len': 48, 'enc_in': 7, 'dec_in': 7, 'c_out': 7, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 3, 'des': 'exp', 'use_gpu': False, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'learning_rate': 0.0001, 'loss': 'mse', 'patience': 3, 'data': 'ETTh1', 'root_path': './ETTDataset/', 'data_path': 'ETTh1.csv', 'train_epochs': 20, 'batch_size': 32, 'pred_len': 24, 'iter': 1, 'detail_freq': 'h'}\n",
      "======================================================================================================================================================\n",
      "Use CPU\n",
      ">>>>>>>start training : informerstack_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_iter1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8521\n",
      "val 2857\n",
      "test 2857\n",
      "\titers: 100, epoch: 1 | loss: 0.4081953\n",
      "\tspeed: 0.7026s/iter; left time: 3668.3653s\n",
      "\titers: 200, epoch: 1 | loss: 0.2965956\n",
      "\tspeed: 0.6864s/iter; left time: 3515.0735s\n",
      "Epoch: 1 cost time: 173.2517867088318\n",
      "Epoch: 1, Steps: 266 | Train Loss: 0.3899380 Vali Loss: 0.6429681 Test Loss: 0.5999037\n",
      "Validation loss decreased (inf --> 0.642968).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2538999\n",
      "\tspeed: 1.1551s/iter; left time: 5723.5755s\n",
      "\titers: 200, epoch: 2 | loss: 0.2431050\n",
      "\tspeed: 0.7100s/iter; left time: 3446.9036s\n",
      "Epoch: 2 cost time: 199.22239995002747\n",
      "Epoch: 2, Steps: 266 | Train Loss: 0.2492066 Vali Loss: 0.7039936 Test Loss: 0.5497743\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1972010\n",
      "\tspeed: 1.9282s/iter; left time: 9041.2493s\n",
      "\titers: 200, epoch: 3 | loss: 0.2181564\n",
      "\tspeed: 0.8013s/iter; left time: 3677.1720s\n",
      "Epoch: 3 cost time: 177.6332197189331\n",
      "Epoch: 3, Steps: 266 | Train Loss: 0.1903314 Vali Loss: 0.7044131 Test Loss: 0.5245351\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1520381\n",
      "\tspeed: 1.7211s/iter; left time: 7612.4086s\n",
      "\titers: 200, epoch: 4 | loss: 0.1452629\n",
      "\tspeed: 0.8833s/iter; left time: 3818.6922s\n",
      "Epoch: 4 cost time: 228.57319498062134\n",
      "Epoch: 4, Steps: 266 | Train Loss: 0.1568208 Vali Loss: 0.7266378 Test Loss: 0.5619211\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "test 2857\n",
      "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
      "test shape: (2848, 24, 7) (2848, 24, 7)\n",
      "mse:0.5995951890945435, mae:0.5722510814666748\n",
      "\n",
      "Moving to next...\n",
      "\n",
      "Training InformerTS on ETTh1 with pred_len 48\n",
      "Beginning to fit the model with the following arguments:\n",
      "{'model': 'informerstack', 'features': 'M', 'target': 'OT', 'freq': 'h', 'checkpoints': './checkpoints', 'seq_len': 96, 'label_len': 48, 'enc_in': 7, 'dec_in': 7, 'c_out': 7, 'factor': 5, 'd_model': 512, 'n_heads': 8, 's_layers': [3, 2, 1], 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 3, 'des': 'exp', 'use_gpu': False, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'learning_rate': 0.0001, 'loss': 'mse', 'patience': 3, 'data': 'ETTh1', 'root_path': './ETTDataset/', 'data_path': 'ETTh1.csv', 'train_epochs': 20, 'batch_size': 32, 'pred_len': 48, 'iter': 1, 'detail_freq': 'h'}\n",
      "======================================================================================================================================================\n",
      "Use CPU\n",
      ">>>>>>>start training : informerstack_ETTh1_ftM_sl96_ll48_pl48_dm512_nh8_el[3, 2, 1]_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_iter1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8497\n",
      "val 2833\n",
      "test 2833\n",
      "\titers: 100, epoch: 1 | loss: 0.4780042\n",
      "\tspeed: 0.5266s/iter; left time: 2738.7235s\n",
      "\titers: 200, epoch: 1 | loss: 0.3814951\n",
      "\tspeed: 0.6118s/iter; left time: 3120.5823s\n",
      "Epoch: 1 cost time: 152.1197464466095\n",
      "Epoch: 1, Steps: 265 | Train Loss: 0.4324473 Vali Loss: 0.7826706 Test Loss: 0.7095712\n",
      "Validation loss decreased (inf --> 0.782671).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.2401508\n",
      "\tspeed: 1.5782s/iter; left time: 7790.0305s\n",
      "\titers: 200, epoch: 2 | loss: 0.2419914\n",
      "\tspeed: 0.4713s/iter; left time: 2279.1497s\n",
      "Epoch: 2 cost time: 177.43645429611206\n",
      "Epoch: 2, Steps: 265 | Train Loss: 0.2579145 Vali Loss: 0.7571150 Test Loss: 0.6415277\n",
      "Validation loss decreased (0.782671 --> 0.757115).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1799057\n",
      "\tspeed: 1.6382s/iter; left time: 7652.2034s\n",
      "\titers: 200, epoch: 3 | loss: 0.1962318\n",
      "\tspeed: 0.7292s/iter; left time: 3333.1174s\n",
      "Epoch: 3 cost time: 202.06809306144714\n",
      "Epoch: 3, Steps: 265 | Train Loss: 0.1926691 Vali Loss: 0.8264337 Test Loss: 0.6958435\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1347973\n",
      "\tspeed: 4.9698s/iter; left time: 21897.1240s\n",
      "\titers: 200, epoch: 4 | loss: 0.1646211\n",
      "\tspeed: 0.6740s/iter; left time: 2902.4426s\n",
      "Epoch: 4 cost time: 506.3180401325226\n",
      "Epoch: 4, Steps: 265 | Train Loss: 0.1639153 Vali Loss: 0.7988588 Test Loss: 0.6481574\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1334703\n",
      "\tspeed: 1.8728s/iter; left time: 7755.4527s\n",
      "\titers: 200, epoch: 5 | loss: 0.1558983\n",
      "\tspeed: 1.0700s/iter; left time: 4323.9644s\n",
      "Epoch: 5 cost time: 254.3843321800232\n",
      "Epoch: 5, Steps: 265 | Train Loss: 0.1490503 Vali Loss: 0.8202145 Test Loss: 0.6849129\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "test 2833\n",
      "test shape: (88, 32, 48, 7) (88, 32, 48, 7)\n",
      "test shape: (2816, 48, 7) (2816, 48, 7)\n",
      "mse:0.6409959197044373, mae:0.5873413681983948\n",
      "\n",
      "Moving to next...\n",
      "\n",
      "CPU times: user 14h 56min 28s, sys: 27min 16s, total: 15h 23min 44s\n",
      "Wall time: 42min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Simple loop\n",
    "for transformermodel in models:\n",
    "    print(f'Model in use: {transformermodel.__name__}')\n",
    "    model = transformermodel()\n",
    "    model.compile(learning_rate=1e-4, loss='mse', early_stopping_patience=3)\n",
    "    for iter in range(1):\n",
    "        for dataset_name, dataset_params in datasets.items():\n",
    "            for pred_len in pred_lens:\n",
    "                print(f\"Training {transformermodel.__name__} on {dataset_name} with pred_len {pred_len}\")\n",
    "\n",
    "                # High amount of epochs to accomodate all models\n",
    "                # Early stopping should kick in anyways\n",
    "                model.fit(\n",
    "                    iter = iter + 1 ,\n",
    "                    data=dataset_name,\n",
    "                    data_root_path=dataset_params[0],\n",
    "                    batch_size=32,\n",
    "                    epochs=20, # very high\n",
    "                    pred_len=pred_len,\n",
    "                    features = dataset_params[1],\n",
    "                    seq_len = dataset_params[2]\n",
    "                )\n",
    "\n",
    "                predictions = model.predict()\n",
    "\n",
    "                print(f'\\nMoving to next...\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {    \n",
    "    'ETTh1': ['./ETTDataset/' , 'M' , 'OT' , 768 , 7 , 7 , 7] ,\n",
    "}\n",
    "pred_lens = [24, 48, 168, 336,720]\n",
    "\n",
    "\n",
    "models = [LogsparseTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model in use: LogsparseTS\n",
      "Training LogsparseTS on ETTh1 with pred_len 24\n",
      "Beginning to fit the model with the following arguments:\n",
      "{'model': 'Logsparse', 'plot_flat': 0, 'verbose': 1, 'is_training': 1, 'inverse': False, 'data': 'ETTh1', 'root_path': './ETTDataset/', 'data_path': 'ETTh1.csv', 'target': 'OT', 'freq': 'h', 'checkpoints': './checkpoints/', 'checkpoint_flag': 1, 'features': 'M', 'seq_len': 768, 'label_len': 48, 'pred_len': 24, 'enc_in': 7, 'dec_in': 7, 'c_out': 7, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'activation': 'gelu', 'output_attention': False, 'win_len': 6, 'res_len': None, 'qk_ker': 4, 'v_conv': 0, 'sparse_flag': 1, 'top_keys': 0, 'kernel_size': 3, 'train_strat_lstm': 'recursive', 'model_id': 'Logsparse_Synth1_24', 'num_workers': 0, 'itr': 3, 'train_epochs': 20, 'batch_size': 32, 'patience': 3, 'learning_rate': 0.0001, 'lr_decay_rate': 0.8, 'des': 'test', 'loss': 'mse', 'lradj': 'type1', 'use_gpu': False, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'iter': 3}\n",
      "======================================================================================================================================================\n",
      "Use CPU\n",
      ">>>>>>>start training : Logsparse_ETTh1_ftM_sl768_ll48_pl24_dm512_nh8_dl1_df2048_fc3_ebtimeF_dtTrue_destest_iter3>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7849\n",
      "val 2857\n",
      "test 2857\n",
      "Could not load best model\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100/245, epoch: 1 | loss: 0.4384453\n",
      "\titers: 200/245, epoch: 1 | loss: 0.3474430\n",
      "Epoch: 1 cost time: 967.793389081955\n",
      "Epoch: 1, Steps: 245 | Train Loss: 0.3235542 Vali Loss: 1.2753967\n",
      "Validation loss decreased (inf --> 1.275397).  Saving model ...\n",
      "Updating learning rate to 8e-05\n",
      "\titers: 100/245, epoch: 2 | loss: 0.1692390\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/Schreibtisch/Github/TransformersEnergyTS/LogSparseAPI.py:2226\u001b[0m, in \u001b[0;36mLogsparseTS.fit\u001b[0;34m(self, data, data_root_path, batch_size, epochs, pred_len, seq_len, features, target, enc_in, dec_in, c_out, iter)\u001b[0m\n\u001b[1;32m   2224\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetting))\n\u001b[0;32m-> 2226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Schreibtisch/Github/TransformersEnergyTS/LogSparseAPI.py:1813\u001b[0m, in \u001b[0;36mExp_Logsparse.train\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m   1810\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x, batch_x_mark, dec_inp, batch_y_mark,\n\u001b[1;32m   1811\u001b[0m                          teacher_forcing_ratio\u001b[38;5;241m=\u001b[39mteacher_forcing_ratio, batch_y\u001b[38;5;241m=\u001b[39mbatch_y)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1813\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[1;32m   1817\u001b[0m     f_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mc_out\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Schreibtisch/Github/TransformersEnergyTS/LogSparseAPI.py:1553\u001b[0m, in \u001b[0;36mLogsparse.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask, dec_self_mask, dec_enc_mask, **_)\u001b[0m\n\u001b[1;32m   1550\u001b[0m attns\u001b[38;5;241m.\u001b[39mappend(a)\n\u001b[1;32m   1552\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_embedding(x_dec, x_mark_dec)\n\u001b[0;32m-> 1553\u001b[0m dec_out, a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_self_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_enc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1554\u001b[0m attns\u001b[38;5;241m.\u001b[39mappend(a)\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_attention:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Schreibtisch/Github/TransformersEnergyTS/LogSparseAPI.py:1474\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m attn \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m-> 1474\u001b[0m     x, a_sa, a_ca \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m     attn\u001b[38;5;241m.\u001b[39mappend(a_sa)\n\u001b[1;32m   1476\u001b[0m     attn\u001b[38;5;241m.\u001b[39mappend(a_ca)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Schreibtisch/Github/TransformersEnergyTS/LogSparseAPI.py:1454\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m   1452\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m-> 1454\u001b[0m x, a_ca \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1455\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m   1457\u001b[0m y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Simple loop\n",
    "for transformermodel in models:\n",
    "    print(f'Model in use: {transformermodel.__name__}')\n",
    "    model = transformermodel()\n",
    "    model.compile(learning_rate=1e-4, loss='mse', early_stopping_patience=3)\n",
    "    for iter in range(2 ,model.args.itr):\n",
    "        for dataset_name, dataset_params in datasets.items():\n",
    "            for pred_len in pred_lens:\n",
    "                print(f\"Training {transformermodel.__name__} on {dataset_name} with pred_len {pred_len}\")\n",
    "\n",
    "                model.fit(\n",
    "                    iter = iter + 1,\n",
    "                    data=dataset_name,\n",
    "                    data_root_path=dataset_params[0],\n",
    "                    batch_size=32,\n",
    "                    epochs=20, \n",
    "                    pred_len=pred_len,\n",
    "                    features = dataset_params[1],\n",
    "                    target = dataset_params[2],\n",
    "                    seq_len = dataset_params[3],\n",
    "                    enc_in = dataset_params[4],\n",
    "                    dec_in = dataset_params[5],\n",
    "                    c_out = dataset_params[6]\n",
    "                )\n",
    "\n",
    "                predictions = model.predict()\n",
    "\n",
    "                print(f'\\nMoving to next...\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
